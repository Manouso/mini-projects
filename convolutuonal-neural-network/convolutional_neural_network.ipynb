{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.amp import autocast, GradScaler  # Updated for modern PyTorch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b8aae",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We'll apply normalization and data augmentation to improve model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2752242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# For training we will use normalization + augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),  # Random rotation for robustness (augmentation)\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation (augmentation)\n",
    "    transforms.ToTensor(), # Convert to tensor (normalization will be applied after)\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std (normalization)\n",
    "])\n",
    "\n",
    "# For testing we use only normalization\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load datasets with transformations from above\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "print(f\"Train set: {len(train_dataset)} samples\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "print(f\"Test set: {len(test_dataset)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64  # Will be tuned later (initial value)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e437bb1",
   "metadata": {},
   "source": [
    "## CNN Model Architecture\n",
    "\n",
    "We'll implement a modern CNN architecture with:\n",
    "- Convolutional layers with different kernel sizes and strides\n",
    "- Batch normalization for stable training\n",
    "- Max pooling for downsampling\n",
    "- Dropout for regularization\n",
    "- Global average pooling before the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # 28x28 -> 28x28\n",
    "        self.bn1 = nn.BatchNorm2d(32) # Batch normalization\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)  # 28x28 -> 28x28\n",
    "        self.bn2 = nn.BatchNorm2d(32) # Batch normalization\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14 (max pooling)\n",
    "        self.dropout1 = nn.Dropout2d(dropout_rate) # Dropout for regularization\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 14x14 -> 14x14\n",
    "        self.bn3 = nn.BatchNorm2d(64) # Batch normalization\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)  # 14x14 -> 14x14\n",
    "        self.bn4 = nn.BatchNorm2d(64) # Batch normalization\n",
    "        self.pool2 = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling: 14x14 -> 1x1\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Fully connected layers (classifier)\n",
    "        self.fc1 = nn.Linear(64, 128) # From 64 (after global avg pool) to 128\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128) # Batch normalization\n",
    "        self.dropout_fc = nn.Dropout(dropout_rate) # Dropout for regularization\n",
    "        self.fc2 = nn.Linear(128, num_classes) # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second block\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layer\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Classifier\n",
    "        x = torch.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Compute total parameters\n",
    "model = MyCNN()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa7009",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Random Search\n",
    "\n",
    "We'll perform random search over key hyperparameters to find the optimal configuration. This is more efficient than grid search for high-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9543e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, loader, criterion, optimizer, scaler, device):  # Added scaler parameter\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        with autocast(device_type=device.type):  # Enable mixed precision\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()  # Scale loss for stability\n",
    "        scaler.step(optimizer)  # Step optimizer\n",
    "        scaler.update()  # Update scaler\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()  # Count correct predictions\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with autocast(device_type=device.type):  # Enable mixed precision for validation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)  # Accumulate loss multiplying by batch size\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()  # Count correct predictions\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Full training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, num_epochs, patience, device):  # Added scaler parameter\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    \n",
    "    # Training loop with early stopping \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device)  # Pass scaler\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc  # Save best model accuracy\n",
    "            best_model_state = model.state_dict()  # Save best model state \n",
    "            patience_counter = 0  # Reset patience counter if the validation accuracy improves\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)  # Load best model state\n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56829d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(num_trials=3, max_epochs=5):\n",
    "    best_accuracy = 0.0\n",
    "    best_params = None\n",
    "    best_model_state = None\n",
    "\n",
    "    # Define hyperparameter ranges\n",
    "    lr_range = [1e-3, 1e-2]\n",
    "    batch_size_range = [32,64]\n",
    "    dropout_range = [0.4, 0.5]\n",
    "    weight_decay = 1e-4\n",
    "    optimizer = 'Adam'\n",
    "    scheduler = 'ReduceLROnPlateau'\n",
    "    patience = 5\n",
    "    results = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        # Randomly sample hyperparameters\n",
    "        lr = random.choice(lr_range)\n",
    "        batch_size = random.choice(batch_size_range)\n",
    "        dropout_rate = random.choice(dropout_range)\n",
    "      \n",
    "        # Create data loaders with current batch size\n",
    "        train_loader_trial = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        val_loader_trial = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        # Initialize model\n",
    "        model = MyCNN(dropout_rate=dropout_rate).to(device)\n",
    "        # Loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        # Scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "        # Scaler for mixed precision\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        # Train for a few epochs\n",
    "        train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "            model, train_loader_trial, val_loader_trial, criterion, optimizer, scheduler, scaler,  # Pass scaler\n",
    "            num_epochs=max_epochs, patience=patience, device=device\n",
    "        )\n",
    "        # Get best validation accuracy\n",
    "        max_val_acc = max(val_accs)\n",
    "        results.append({\n",
    "            'trial': trial+1,\n",
    "            'lr': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'optimizer': 'Adam',\n",
    "            'scheduler': 'ReduceLROnPlateau',\n",
    "            'patience': patience,\n",
    "            'val_accuracy': max_val_acc\n",
    "        })\n",
    "        if max_val_acc > best_accuracy:\n",
    "            best_accuracy = max_val_acc\n",
    "            best_params = {\n",
    "                'lr': lr,\n",
    "                'batch_size': batch_size,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'weight_decay': weight_decay,\n",
    "                'optimizer': optimizer,\n",
    "                'scheduler': scheduler,\n",
    "                'patience': patience\n",
    "            }\n",
    "            best_model_state = model.state_dict()\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    return best_params, best_model_state, results\n",
    "\n",
    "# Run random search\n",
    "best_params, best_model_state, tuning_results = random_search(num_trials=3, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd6efe",
   "metadata": {},
   "source": [
    "## Final Model Training\n",
    "\n",
    "Train the model with the best hyperparameters found from random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "final_model = MyCNN(dropout_rate=best_params['dropout_rate']).to(device)\n",
    "if best_model_state:\n",
    "    final_model.load_state_dict(best_model_state)\n",
    "\n",
    "# Create data loaders with best batch size\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=0)\n",
    "final_val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=0)\n",
    "final_test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "# Loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "# Scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Train the final model\n",
    "print(\"Training final model...\")\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    final_model, final_train_loader, final_val_loader, criterion, optimizer, scheduler, scaler,  # Pass scaler\n",
    "    num_epochs=20, patience=5, device=device\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00255c85",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with autocast(device_type=device.type):  # Enable mixed precision for evaluation\n",
    "                outputs = model(inputs)\n",
    "            _, preds = outputs.max(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "test_preds, test_labels = evaluate_model(final_model, final_test_loader, device)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = np.mean(test_preds == test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=[str(i) for i in range(10)]))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Show some correctly classified examples\n",
    "correct_indices = np.where(test_preds == test_labels)[0][:10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, idx in enumerate(correct_indices):\n",
    "    img, true_label = test_dataset[idx]\n",
    "    pred_label = test_preds[idx]\n",
    "\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
