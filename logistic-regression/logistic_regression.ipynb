{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7203b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import roc_auc_score, PrecisionRecallDisplay, RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import learning_curve, validation_curve, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, FunctionTransformer, TargetEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import loguniform,rankdata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc02633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/titanic_preprocessed.csv')\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "X = data.drop('Survived', axis=1) # We `drop` the target column from features\n",
    "y = data['Survived']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Splitting the data into training and testing sets 20% test size 80% train size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=5000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # Ensures each fold has the same proportion of classes as the whole dataset\n",
    "\n",
    "# Defining hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga'], # solvers that support both l1 and l2 penalties\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Defining hyperparameter distribution for Randomized Search\n",
    "param_distributions = {\n",
    "    'C': loguniform(1e-3,1e3),\n",
    "    'penalty': ['l1','l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1, # Use all available cores\n",
    "    verbose=1 # Print progress messages\n",
    ")\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    n_iter=50, # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Measure the time taken for both searches\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Grid Search took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Randomized Search took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "grid_model = grid_search.best_estimator_  # Best model from Grid Search\n",
    "random_model = random_search.best_estimator_  # Best model from Randomized Search\n",
    "\n",
    "y_pred_grid = grid_model.predict(X_test)\n",
    "y_pred_random = random_model.predict(X_test)\n",
    "\n",
    "# Evaluating Grid Search model\n",
    "print(\"Grid Search Model Evaluation:\")\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_grid))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_grid))\n",
    "\n",
    "# Evaluating Randomized Search model\n",
    "print(\"\\nRandomized Search Model Evaluation:\")\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_random))\n",
    "\n",
    "# Confusion Matrix: actual vs predicted shows counts of TN, FP, FN, TP\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_random))\n",
    "\n",
    "# Accuracy: how many predictions were correct\n",
    "# Precision: of all positive predictions, how many were actually positive\n",
    "# Recall: of all actual positives, how many were correctly predicted\n",
    "# F1-Score: harmonic mean of precision and recall\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_random))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_random)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix of Logistic Regression Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance: coefficients of the logistic regression model\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': random_model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=coefficients, x='Coefficient', y='Feature')\n",
    "plt.title(\"Feature Importance (Logistic Regression Coefficients)\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve: model performance vs training set size\n",
    "learning_curves = learning_curve(\n",
    "    estimator=random_model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curves\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Accuracy')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Validation Accuracy')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Curve: model performance vs hyperparameter values\n",
    "param_range = np.logspace(-3, 3, 7)\n",
    "validation_curves = validation_curve(\n",
    "    estimator=random_model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    param_name='C',\n",
    "    param_range=param_range,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores, test_scores = validation_curves\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training Accuracy')\n",
    "plt.semilogx(param_range, test_scores_mean, label='Validation Accuracy')\n",
    "plt.xlabel('C (Inverse of Regularization Strength)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis (Visual Only)\n",
    "if hasattr(random_model, \"predict_proba\"):\n",
    "    y_proba = random_model.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(random_model, \"decision_function\"):\n",
    "    scores = random_model.decision_function(X_test)\n",
    "    ranks = pd.Series(scores, index=y_test.index).rank(method=\"average\")\n",
    "    y_proba = ((ranks - ranks.min()) / (ranks.max() - ranks.min())).to_numpy()\n",
    "else:\n",
    "    y_proba = y_pred_random.astype(float)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name=\"LogReg\").plot(ax=axes[0])\n",
    "axes[0].set_title(f\"ROC Curve (AUC={auc:.3f})\")\n",
    "PrecisionRecallDisplay(precision=prec, recall=rec, estimator_name=\"LogReg\").plot(ax=axes[1])\n",
    "axes[1].set_title(\"Precision–Recall Curve\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef405dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanations\n",
    "sample_size = 200  # adjust for speed vs detail\n",
    "background = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "eval_sample = X_test.sample(min(sample_size, len(X_test)), random_state=42)\n",
    "explainer = shap.LinearExplainer(random_model, background)\n",
    "shap_values = explainer.shap_values(eval_sample)\n",
    "\n",
    "shap.summary_plot(shap_values, eval_sample, plot_type='bar', show=False)\n",
    "plt.title('Mean Absolute SHAP Value (Global Importance)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP on misclassifications (FP/FN): which features drive the mistakes?\n",
    "\n",
    "sample_size = 200\n",
    "background = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "explainer = shap.LinearExplainer(random_model, background)\n",
    "\n",
    "# Evaluate on the full test set (or sample for speed)\n",
    "eval_df = X_test.copy()\n",
    "y_true = y_test.loc[eval_df.index]\n",
    "y_pred = random_model.predict(eval_df)\n",
    "\n",
    "# SHAP values for the evaluation slice\n",
    "shap_vals = explainer.shap_values(eval_df)\n",
    "if isinstance(shap_vals, list):  # some SHAP versions return list per class\n",
    "    shap_vals = shap_vals[1] if len(shap_vals) > 1 else shap_vals[0]\n",
    "\n",
    "shap_df = pd.DataFrame(shap_vals, index=eval_df.index, columns=eval_df.columns)\n",
    "\n",
    "# Masks for error types\n",
    "fp_mask = (y_true == 0) & (y_pred == 1)\n",
    "fn_mask = (y_true == 1) & (y_pred == 0)\n",
    "correct_mask = ~fp_mask & ~fn_mask\n",
    "\n",
    "# Aggregate mean absolute contributions\n",
    "mean_abs_fp = np.abs(shap_df[fp_mask.values]).mean() if fp_mask.any() else pd.Series(0, index=shap_df.columns)\n",
    "mean_abs_fn = np.abs(shap_df[fn_mask.values]).mean() if fn_mask.any() else pd.Series(0, index=shap_df.columns)\n",
    "mean_abs_correct = np.abs(shap_df[correct_mask.values]).mean() if correct_mask.any() else pd.Series(0, index=shap_df.columns)\n",
    "\n",
    "# Directional differences (error vs correct)\n",
    "signed_fp = shap_df[fp_mask.values].mean() if fp_mask.any() else pd.Series(0, index=shap_df.columns)\n",
    "signed_fn = shap_df[fn_mask.values].mean() if fn_mask.any() else pd.Series(0, index=shap_df.columns)\n",
    "signed_correct = shap_df[correct_mask.values].mean() if correct_mask.any() else pd.Series(0, index=shap_df.columns)\n",
    "\n",
    "# Features whose absolute impact is larger in errors than in correct predictions are suspect\n",
    "error_overuse_score = ((mean_abs_fp + mean_abs_fn)/2 - mean_abs_correct).sort_values(ascending=False)\n",
    "\n",
    "# Plot top 5 by error_overuse_score\n",
    "k = min(5, len(error_overuse_score))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "error_overuse_score.head(k).plot(kind='barh', ax=axes[0], color='tomato')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_title('Features over-emphasized in errors (|SHAP| error − |SHAP| correct)')\n",
    "axes[0].set_xlabel('Delta mean |SHAP|')\n",
    "\n",
    "# Directionality: where errors systematically push the wrong way vs correct\n",
    "delta_fp = (signed_fp - signed_correct)\n",
    "delta_fn = (signed_fn - signed_correct)\n",
    "\n",
    "# Combine by max abs per feature to highlight strongest misdirection\n",
    "misdirection = pd.concat({'FP': delta_fp, 'FN': delta_fn}, axis=1)\n",
    "misdirection['max_abs'] = misdirection.abs().max(axis=1)\n",
    "misdirection_sorted = misdirection.sort_values('max_abs', ascending=False).drop(columns=['max_abs'])\n",
    "\n",
    "misdirection_sorted.head(k).plot(kind='barh', ax=axes[1])\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_title('Signed SHAP delta (error − correct): FP and FN')\n",
    "axes[1].set_xlabel('Mean SHAP difference')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top potentially harmful-by-overuse features:')\n",
    "print(error_overuse_score.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis and Visualization\n",
    "\n",
    "new_data = pd.read_csv('../datasets/Titanic-Dataset.csv')\n",
    "print(new_data.info())\n",
    "print(new_data.isna().sum())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Visualizing Survival Count by Embarkation Point\n",
    "sns.countplot(data=new_data, x='Survived', hue='Embarked', ax=axes[0])\n",
    "axes[0].set_title('Survival Count by Embarkation Point')\n",
    "axes[0].set_xlabel('Survived')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Visualizing Survival Count by Sex\n",
    "sns.countplot(data=new_data, x='Survived', hue='Sex', ax=axes[1])\n",
    "axes[1].set_title('Survival Count by Sex')\n",
    "axes[1].set_xlabel('Survived')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(title='Sex')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'Survived', 'Pclass']\n",
    "\n",
    "# Heatmap of Correlation Matrix\n",
    "plt.figure(figsize=(12,8))\n",
    "corr_matrix = new_data[numerical_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Functions (for FunctionTransformer)\n",
    "\n",
    "def drop_and_impute(X):\n",
    "    \"\"\"Drop irrelevant columns and impute missing values\"\"\"\n",
    "    X = X.copy()\n",
    "    X = X.drop(['Name', 'Ticket', 'PassengerId'], axis=1, errors='ignore')\n",
    "    X['Age'] = X['Age'].fillna(X['Age'].median())\n",
    "    X['Cabin'] = X['Cabin'].fillna('Unknown')\n",
    "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0] if not X['Embarked'].mode().empty else 'S')\n",
    "    return X\n",
    "\n",
    "def create_features(X):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Family features\n",
    "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n",
    "    X['IsAlone'] = (X['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # Age grouping\n",
    "    X['AgeGroup'] = pd.cut(X['Age'], bins=[-1, 12, 20, 40, 60, 100], labels=[0,1,2,3,4]).astype(int)\n",
    "    \n",
    "    # Fare grouping\n",
    "    X['FareGroup'] = pd.qcut(X['Fare'], 4, labels=[0,1,2,3]).astype(int)\n",
    "    \n",
    "    # Fare per person\n",
    "    X['FarePerPerson'] = X['Fare'] / X['FamilySize']\n",
    "    X['FarePerPerson'] = X['FarePerPerson'].fillna(X['Fare'])\n",
    "    \n",
    "    # Encode Sex and Cabin\n",
    "    X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n",
    "    X['Cabin'] = X['Cabin'].apply(lambda x: 0 if x == 'Unknown' else 1)\n",
    "    \n",
    "    # Interaction features\n",
    "    X['Sex_Pclass'] = X['Sex'] * X['Pclass']\n",
    "    X['Age_Pclass'] = X['Age'] * X['Pclass']\n",
    "    X['Fare_Pclass'] = X['Fare'] * X['Pclass']\n",
    "\n",
    "    # Log Transformations\n",
    "    X['Log_Fare'] = np.log1p(X['Fare'])\n",
    "    X['Log_Age'] = np.log1p(X['Age'])\n",
    "    X['Log_FarePerPerson'] = np.log1p(X['FarePerPerson'])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full ML Pipeline with No Data Leakage\n",
    "\n",
    "X = new_data.drop('Survived', axis=1)\n",
    "y = new_data['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the pipeline - all steps fit only on training data\n",
    "pipeline = Pipeline([\n",
    "    # Step 1: Drop and impute\n",
    "    ('drop_impute', FunctionTransformer(drop_and_impute)),\n",
    "    \n",
    "    # Step 2: Create new features\n",
    "    ('create_features', FunctionTransformer(create_features)),\n",
    "    \n",
    "    # Step 3: One-hot encode Embarked (proper encoder in pipeline)\n",
    "    ('encoder', ColumnTransformer([\n",
    "        ('embarked', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), ['Embarked'])\n",
    "        # verbose_feature_names_out=False means original names are kept so we can identify features easily\n",
    "    ], remainder='passthrough', verbose_feature_names_out=False)), # remainder='passthrough' keeps other columns unchanged \n",
    "   \n",
    "    \n",
    "    # Step 4: Scale numerical features (fit only on training data)\n",
    "    ('scaler', StandardScaler()),\n",
    "    \n",
    "    # Step 5: Logistic Regression model\n",
    "    ('model', LogisticRegression(max_iter=5000, tol=1e-3, random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning with pipeline\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_distributions_pipeline = {\n",
    "    'model__C': loguniform(1e-3, 1e3),\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "random_search_eng = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions_pipeline,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1, \n",
    "    verbose=0,\n",
    "    n_iter=50, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search_eng.fit(X_train, y_train)\n",
    "print(f\"Randomized Search took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Get best model\n",
    "model_eng = random_search_eng.best_estimator_\n",
    "y_pred_eng = model_eng.predict(X_test)\n",
    "y_proba_eng = model_eng.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nEnhanced Model with Feature Engineering Pipeline:\")\n",
    "print(\"Best Hyperparameters:\", random_search_eng.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_eng))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_eng))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "\n",
    "# Baseline model (from cell 3)\n",
    "baseline_acc = accuracy_score(y_test, y_pred_random)\n",
    "baseline_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Enhanced model\n",
    "enhanced_acc = accuracy_score(y_test, y_pred_eng)\n",
    "enhanced_auc = roc_auc_score(y_test, y_proba_eng)\n",
    "\n",
    "# Performance comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# ROC Curves\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_proba)\n",
    "fpr_enh, tpr_enh, _ = roc_curve(y_test, y_proba_eng)\n",
    "axes[0,0].plot(fpr_base, tpr_base, label=f'Baseline (AUC={baseline_auc:.3f})', linewidth=2)\n",
    "axes[0,0].plot(fpr_enh, tpr_enh, label=f'Enhanced (AUC={enhanced_auc:.3f})', linewidth=2)\n",
    "axes[0,0].plot([0,1], [0,1], 'k--', alpha=0.3)\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].set_title('ROC Curve Comparison')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "models = ['Baseline', 'Enhanced']\n",
    "accuracies = [baseline_acc, enhanced_acc]\n",
    "colors = ['blue', 'red']\n",
    "axes[0,1].bar(models, accuracies, color=colors, alpha=0.7)\n",
    "axes[0,1].set_ylabel('Accuracy')\n",
    "axes[0,1].set_title('Accuracy Comparison')\n",
    "axes[0,1].set_ylim([0.7, 0.95])\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0,1].text(i, v+0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Validation Curve for Enhanced Model (using pipeline)\n",
    "param_range = np.logspace(-3, 3, 7)\n",
    "vc_eng = validation_curve(\n",
    "    estimator=model_eng,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    param_name='model__C',\n",
    "    param_range=param_range,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores_eng, test_scores_eng = vc_eng\n",
    "train_mean_eng = np.mean(train_scores_eng, axis=1)\n",
    "test_mean_eng = np.mean(test_scores_eng, axis=1)\n",
    "axes[0,2].semilogx(param_range, train_mean_eng, label='Training', linewidth=2)\n",
    "axes[0,2].semilogx(param_range, test_mean_eng, label='Validation', linewidth=2)\n",
    "axes[0,2].set_xlabel('C (Inverse of Regularization Strength)')\n",
    "axes[0,2].set_ylabel('Accuracy')\n",
    "axes[0,2].set_title('Validation Curve (Enhanced Model)')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(alpha=0.3)\n",
    "\n",
    "# Learning curve for enhanced model (using pipeline)\n",
    "lc_eng = learning_curve(model_eng, X_train, y_train, cv=skf, \n",
    "                         scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "train_sizes_eng, train_scores_eng, test_scores_eng = lc_eng\n",
    "train_mean_eng = np.mean(train_scores_eng, axis=1)\n",
    "test_mean_eng = np.mean(test_scores_eng, axis=1)\n",
    "\n",
    "axes[1,0].plot(train_sizes_eng, train_mean_eng, label='Training', linewidth=2)\n",
    "axes[1,0].plot(train_sizes_eng, test_mean_eng, label='Validation', linewidth=2)\n",
    "axes[1,0].set_xlabel('Training Set Size')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].set_title('Learning Curve (Enhanced Model)')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "# Feature importance (top 15) \n",
    "final_model = model_eng.named_steps['model']\n",
    "\n",
    "# Get feature names by transforming data and tracking columns\n",
    "X_sample = X_train.head(10)\n",
    "X_after_features = model_eng.named_steps['create_features'].transform(\n",
    "    model_eng.named_steps['drop_impute'].transform(X_sample)\n",
    ")\n",
    "base_features = list(X_after_features.columns)\n",
    "\n",
    "# After one-hot encoding Embarked\n",
    "encoder = model_eng.named_steps['encoder']\n",
    "embarked_cats = encoder.named_transformers_['embarked'].categories_[0][1:]  # drop first\n",
    "embarked_features = [f'Embarked_{cat}' for cat in embarked_cats]\n",
    "\n",
    "# Combine all feature names\n",
    "all_features = embarked_features + [f for f in base_features if f != 'Embarked']\n",
    "feature_names = all_features\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': final_model.coef_[0]\n",
    "}).sort_values(by='Coefficient', key=abs, ascending=False).head(15)\n",
    "\n",
    "axes[1,1].barh(coef_df['Feature'], coef_df['Coefficient'], color='#2ecc71', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Coefficient')\n",
    "axes[1,1].set_title('Top 15 Feature Importances')\n",
    "axes[1,1].invert_yaxis()\n",
    "axes[1,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Confusion Matrix Delta (Enhanced - Baseline)\n",
    "cm_base = confusion_matrix(y_test, y_pred_random)\n",
    "cm_enh = confusion_matrix(y_test, y_pred_eng)\n",
    "cm_delta = cm_enh - cm_base\n",
    "sns.heatmap(cm_delta, annot=True, fmt='d', cmap='RdYlGn', center=0, ax=axes[1,2], cbar_kws={'label': 'Change'})\n",
    "axes[1,2].set_xlabel('Predicted')\n",
    "axes[1,2].set_ylabel('Actual')\n",
    "axes[1,2].set_title('Confusion Matrix Delta\\n(Enhanced - Baseline)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
