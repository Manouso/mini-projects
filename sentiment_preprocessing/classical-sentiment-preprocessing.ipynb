{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the English model for spaCy\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df.head())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable componenents to speed up processing and focus on tokenization and lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Process reviews efficiently using streaming (generator) to avoid memory overflow on large datasets\n",
    "def preprocess_dataset(texts):\n",
    "    # Remove HTML tags first\n",
    "    texts = [re.sub(r'<[^>]+>', '', text) for text in texts]\n",
    "    preprocessed_text = []\n",
    "    # nlp.pipe processes texts as a stream and is much faster than applying nlp to each text individually\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        # Extract lemmas for non-stop, non-punct, alphabetic tokens, lowercased\n",
    "        tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "        preprocessed_text.append(' '.join(tokens))\n",
    "    return preprocessed_text\n",
    "            \n",
    "\n",
    "# Display original and preprocessed text for the first few reviews to verify preprocessing\n",
    "# Note: Preprocessing will be done after train/test split to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary & Vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize vectorizers with n-gram support\n",
    "bow_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=5, max_features=5000) # ngram_range=(1,2) means unigrams + bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=5000) # min_df=5 ignores terms that appear in less than 5 documents (removes noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization Phase\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode labels first (positive = 1, negative = 0)\n",
    "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Split data into train/test (80/20) to prevent data leakage\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df['review'], \n",
    "    df['label'], \n",
    "    test_size=0.2, \n",
    "    stratify=df['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess training and test sets separately\n",
    "X_train_text = preprocess_dataset(X_train_text)\n",
    "X_test_text = preprocess_dataset(X_test_text)\n",
    "\n",
    "# Fit vectorizers on training data only, then transform both sets\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train_text)\n",
    "X_test_bow = bow_vectorizer.transform(X_test_text)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f\"Training set size: {len(X_train_text)}\")\n",
    "print(f\"Test set size: {len(X_test_text)}\")\n",
    "print(f\"BoW Matrix Shape (train): {X_train_bow.shape}\")\n",
    "print(f\"TF-IDF Matrix Shape (train): {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Pipeline with Cross-Validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "# Set up Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "model_tfidf = LogisticRegressionCV(\n",
    "    max_iter=1000, \n",
    "    Cs=20, \n",
    "    solver='saga', \n",
    "    n_jobs=-1, \n",
    "    cv=skf,\n",
    "    random_state=42,\n",
    "    l1_ratios=(0,),\n",
    "    use_legacy_attributes=True\n",
    ")\n",
    "\n",
    "model_bow = LogisticRegressionCV(\n",
    "    max_iter=1000, \n",
    "    Cs=20, \n",
    "    solver='saga', \n",
    "    n_jobs=-1, \n",
    "    cv=skf,\n",
    "    random_state=42,\n",
    "    l1_ratios=(0,),\n",
    "    use_legacy_attributes=True\n",
    ")\n",
    "\n",
    "model_tfidf_alt = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "model_bow_alt = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "# Train all models on training set\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "model_tfidf_alt.fit(X_train_tfidf, y_train)\n",
    "model_bow_alt.fit(X_train_bow, y_train)\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "print(\"TF-IDF Logistic Regression Test Results:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "print(confusion_matrix(y_test, y_pred_tfidf))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, model_tfidf.predict_proba(X_test_tfidf)[:, 1]):.4f}\")\n",
    "\n",
    "y_pred_tfidf_alt = model_tfidf_alt.predict(X_test_tfidf)\n",
    "print(\"TF-IDF Naive Bayes Test Results:\")\n",
    "print(classification_report(y_test, y_pred_tfidf_alt))\n",
    "print(confusion_matrix(y_test, y_pred_tfidf_alt))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, model_tfidf_alt.predict_proba(X_test_tfidf)[:, 1]):.4f}\")\n",
    "\n",
    "y_pred_bow_alt = model_bow_alt.predict(X_test_bow)\n",
    "print(\"BoW Naive Bayes Test Results:\")\n",
    "print(classification_report(y_test, y_pred_bow_alt))\n",
    "print(confusion_matrix(y_test, y_pred_bow_alt))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, model_bow_alt.predict_proba(X_test_bow)[:, 1]):.4f}\")\n",
    "\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "print(\"BoW Logistic Regression Test Results:\")\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "print(confusion_matrix(y_test, y_pred_bow))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, model_bow.predict_proba(X_test_bow)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ea82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models and vectorizers for deployment\n",
    "with open('sentiment_models.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'tfidf_model': model_tfidf,\n",
    "        'tfidf_model_alt': model_tfidf_alt,\n",
    "        'bow_model': model_bow,\n",
    "        'bow_model_alt': model_bow_alt,\n",
    "        'tfidf_vectorizer': tfidf_vectorizer,\n",
    "        'bow_vectorizer': bow_vectorizer\n",
    "    }, f)\n",
    "\n",
    "print(\"All models and vectorizers saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
