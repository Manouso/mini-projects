{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary data\n",
    "import spacy.cli\n",
    "spacy.cli.download('en_core_web_sm') # Download English model\n",
    "\n",
    "# Load the IMDB dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the raw data\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])  # Load spacy without unnecessary components (for this project) for efficiency\n",
    "\n",
    "# Preprocessing function to clean reviews\n",
    "def preprocess_dataset(texts):\n",
    "    cleaned_texts = []\n",
    "    docs = list(nlp.pipe(texts, batch_size=1000))\n",
    "    for doc in docs:\n",
    "        # Collect lemmas for valid tokens in this doc\n",
    "        cleaned = [token.lemma_ for token in doc if not token.is_stop and (token.is_alpha or token.is_digit)]\n",
    "        # Join into a single string per review\n",
    "        cleaned_texts.append(' '.join(cleaned))\n",
    "    return cleaned_texts\n",
    "\n",
    "# Apply preprocessing to the reviews\n",
    "df['cleaned_review'] = preprocess_dataset(df['review'])\n",
    "print(df[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams from cleaned text\n",
    "def generate_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Apply n-gram generation (bigrams)\n",
    "df['bigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 2))\n",
    "print(df[['cleaned_review', 'bigrams']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe word embeddings\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Split df to avoid data leakage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.15, stratify=df['sentiment'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.176, stratify=temp_df['sentiment'], random_state=42)  # ~15% val from remaining\n",
    "\n",
    "print(\"Data split complete.\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Prepare data for neural network using pretrained GloVe embeddings (vocab from train only)\n",
    "\n",
    "# Use cleaned_review without special tokens\n",
    "train_df['label'] = train_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "val_df['label'] = val_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "test_df['label'] = test_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Define special tokens for sequence modeling\n",
    "special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "\n",
    "# Get all unique words from train_df only\n",
    "all_words = set()\n",
    "for review in train_df['cleaned_review']:\n",
    "    words = review.split()\n",
    "    all_words.update(words)\n",
    "\n",
    "# The vocabulary contains all the necessary words and tokens for the embeddings (from train)\n",
    "vocab = special_tokens + list(all_words)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Every word should be assigned with an index for vectorization \n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create embedding matrix (from train vocab)\n",
    "embedding_dim = 100\n",
    "# vocab size (x) x embedding dimensions (y) \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# The embedding matrix has all the words that exist in both the vocabulary of the train dataset and pre-trained model\n",
    "for word, i in word_to_index.items():\n",
    "    if word in model and word not in special_tokens:\n",
    "        # Returns vector with 100 values\n",
    "        embedding_matrix[i] = model[word]\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(reviews, word_to_index):\n",
    "    X = []\n",
    "    for review in reviews:\n",
    "        # Assigns index of SOS to begin and if a word doesnt exist it assigns index of UNK and in the end EOS \n",
    "        seq = [word_to_index['<SOS>']] + [word_to_index.get(word, word_to_index['<UNK>']) for word in review.split()] + [word_to_index['<EOS>']]\n",
    "        X.append(seq)\n",
    "    max_len = max(len(seq) for seq in X) if X else 0\n",
    "    X = np.array([seq + [word_to_index['<PAD>']] * (max_len - len(seq)) for seq in X])\n",
    "    return X\n",
    "\n",
    "# Convert to sequences for train, val, test\n",
    "X_train = create_sequences(train_df['cleaned_review'], word_to_index)\n",
    "X_val = create_sequences(val_df['cleaned_review'], word_to_index)\n",
    "X_test = create_sequences(test_df['cleaned_review'], word_to_index)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "all_seqs = [seq for seqs in [X_train, X_val, X_test] for seq in seqs]\n",
    "max_len = max(len(seq) for seq in all_seqs) if all_seqs else 0\n",
    "\n",
    "# Pad all sequences to the global max_len\n",
    "def pad_to_max_len(X, max_len, pad_idx):\n",
    "    result = []\n",
    "    for seq in X:\n",
    "        if len(seq) < max_len:\n",
    "            pads = np.full(max_len - len(seq), pad_idx, dtype=seq.dtype)\n",
    "            result.append(np.concatenate([seq, pads]))\n",
    "        else:\n",
    "            result.append(seq)\n",
    "    return np.array(result)\n",
    "\n",
    "X_train = pad_to_max_len(X_train, max_len, word_to_index['<PAD>'])\n",
    "X_val = pad_to_max_len(X_val, max_len, word_to_index['<PAD>'])\n",
    "X_test = pad_to_max_len(X_test, max_len, word_to_index['<PAD>'])\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "with open('neural_preprocessing.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'word_to_index': word_to_index,\n",
    "        'embedding_matrix': embedding_matrix,\n",
    "        'max_len': max_len,\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test\n",
    "    }, f)\n",
    "\n",
    "print(\"Neural preprocessing complete. Data saved for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74993f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Load preprocessed data\n",
    "with open('neural_preprocessing.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    word_to_index = data['word_to_index']\n",
    "    embedding_matrix = data['embedding_matrix']\n",
    "    max_len = data['max_len']\n",
    "    X_train = data['X_train']\n",
    "    X_val = data['X_val']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_val = data['y_val']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.from_numpy(X_train).long() \n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_val = torch.from_numpy(X_val).long()\n",
    "y_val = torch.from_numpy(y_val).float()\n",
    "X_test = torch.from_numpy(X_test).long()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Model for Sentimantic Analysis\n",
    "class SemanticClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        weights = torch.as_tensor(embedding_matrix, dtype=torch.float)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "        \n",
    "        # Concatenated Mean + Max pooling = 2 * embedding_dim\n",
    "        input_dim = weights.shape[1] * 2 \n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),  # Changed to LayerNorm for better stability in NLP tasks\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        mask = (x != 0).unsqueeze(-1).float()\n",
    "        \n",
    "        # Masked Mean Pooling\n",
    "        mean_pooled = (embedded * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        # Masked Max Pooling (set padding to very low value)\n",
    "        max_pooled, _ = torch.max(embedded * mask + (1 - mask) * -1e9, dim=1)\n",
    "        \n",
    "        # Combine both to capture average sentiment AND peak signals\n",
    "        combined = torch.cat([mean_pooled, max_pooled], dim=1)\n",
    "        return self.net(combined).squeeze()\n",
    "\n",
    "# Implement GRU and LSTM to compare them with the other neural and classical approaches\n",
    "\n",
    "# Optimized RNN Base (Used for both GRU and LSTM)\n",
    "class OptimizedRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        weights = torch.as_tensor(embedding_matrix, dtype=torch.float)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "        \n",
    "        # Added Bidirectionality (looks at context from both ends)\n",
    "        # Added Multiple Layers\n",
    "        RNN_Class = nn.GRU if rnn_type == \"GRU\" else nn.LSTM\n",
    "        self.rnn = RNN_Class(\n",
    "            weights.shape[1], \n",
    "            hidden_dim, \n",
    "            num_layers=1, \n",
    "            bidirectional=True, \n",
    "            batch_first=True, \n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # Input is hidden_dim * 2 (bidirectional)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),  # Changed to LayerNorm\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Use torch.sum on current device\n",
    "        lengths = (x != 0).sum(dim=1).to(torch.int64).cpu() \n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Use full output instead of just hidden state\n",
    "        packed_output, _ = self.rnn(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Global Max Pooling over time: Captures the most important feature found in the sequence\n",
    "        pooled, _ = torch.max(output, dim=1)\n",
    "        \n",
    "        return self.fc(pooled).squeeze()\n",
    "\n",
    "\n",
    "class RNNwAttention(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        weights = torch.as_tensor(embedding_matrix, dtype=torch.float)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "\n",
    "        RNN_Class = nn.GRU if rnn_type == \"GRU\" else nn.LSTM\n",
    "        self.rnn = RNN_Class(\n",
    "            weights.shape[1], \n",
    "            hidden_dim, \n",
    "            num_layers=1, \n",
    "            bidirectional=True, \n",
    "            batch_first=True, \n",
    "            dropout=0\n",
    "        )        \n",
    "\n",
    "        # Attention layer: linear projection for attention scores\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1) # hidden_dim * 2 due to bidirectionality\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),  # Changed to LayerNorm\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lengths = (x != 0).sum(dim=1).to(torch.int64).cpu() \n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, _ = self.rnn(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Attention mechanism\n",
    "        att_scores = self.attention(output).squeeze(-1)  # (batch, seq_len_batch_max)\n",
    "        # Create mask based on actual lengths\n",
    "        mask = torch.arange(output.size(1)).unsqueeze(0).to(x.device) < lengths.unsqueeze(1).to(x.device)  # (batch, seq_len_batch_max)\n",
    "        att_scores = att_scores.masked_fill(mask == 0, -1e9)  # Use -1e9 to ignore padding in softmax\n",
    "        att_weights = torch.softmax(att_scores, dim=1).unsqueeze(-1)  # (batch, seq_len_batch_max, 1)\n",
    "        attended = (output * att_weights).sum(dim=1)  # (batch, hidden_dim * 2)\n",
    "        \n",
    "        return self.fc(attended).squeeze(), att_weights.squeeze(-1)  # Return att_weights as (batch, seq_len_batch_max) for consistency\n",
    "        \n",
    "\n",
    "# GRU approach (faster, simpler)\n",
    "class SemanticClassifierGRU(OptimizedRNN):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__(embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"GRU\")\n",
    "\n",
    "# LSTM approach (more reliable)\n",
    "class SemanticClassifierLSTM(OptimizedRNN):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__(embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"LSTM\")\n",
    "\n",
    "# GRU approach with Attention \n",
    "class AttwSemanticClassifierGRU(RNNwAttention):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__(embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"GRU\")\n",
    "\n",
    "# LSTM approach with Attention\n",
    "class AttwSemanticClassifierLSTM(RNNwAttention):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout):\n",
    "        super().__init__(embedding_matrix, hidden_dim, output_dim, dropout, rnn_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "def train_and_evaluate_model(model_class, embedding_matrix, train_loader, val_loader, test_loader, lr=0.001, weight_decay=1e-3, hidden_dim=64, dropout=0.5):\n",
    "    model = model_class(embedding_matrix, hidden_dim=hidden_dim, output_dim=1, dropout=dropout)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # LR decay\n",
    "    criterion = nn.BCEWithLogitsLoss()  # More stable\n",
    "    patience = 0\n",
    "    best_val_loss = float('inf')  # Track best val loss for early stopping\n",
    "\n",
    "    # Training with 20 epochs\n",
    "    epochs = 20\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]  # Unpack for attention models\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()  # Update LR\n",
    "        \n",
    "        # Validation on val_loader\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                val_output = model(batch_X)\n",
    "                if isinstance(val_output, tuple):\n",
    "                    val_output = val_output[0]  # Unpack for attention models\n",
    "                loss = criterion(val_output, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                preds = (val_output > 0).int()  # Note: > 0 since logits, not probs\n",
    "                all_preds.extend(preds.numpy())\n",
    "                all_labels.extend(batch_y.int().numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {train_loss/len(train_loader):.4f}, Val Loss {val_loss/len(val_loader):.4f}, Acc {acc:.4f}, Prec {prec:.4f}, Rec {rec:.4f}, F1 {f1:.4f}\")\n",
    "\n",
    "        # Early stopping based on val loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 2:\n",
    "                break\n",
    "    \n",
    "    return model, best_val_loss\n",
    "\n",
    "def tune_hyperparameters(model_class, embedding_matrix, train_loader, val_loader, test_loader):\n",
    "    param_space = {\n",
    "        'lr': [0.001, 0.0005],\n",
    "        'weight_decay': [1e-3, 1e-2],\n",
    "        'dropout': [0.3, 0.5],\n",
    "        'hidden_dim': [32, 64]\n",
    "    }\n",
    "    \n",
    "    # Generate all possible combinations\n",
    "    all_combos = list(product(\n",
    "        param_space['lr'],\n",
    "        param_space['weight_decay'],\n",
    "        param_space['dropout'],\n",
    "        param_space['hidden_dim']\n",
    "    ))\n",
    "    \n",
    "    # Randomly sample 5 combinations\n",
    "    sampled_combos = random.sample(all_combos, 5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model_state = None\n",
    "    for combo in sampled_combos:\n",
    "        lr, wd, drop, hd = combo\n",
    "        print(f\"Trying lr={lr}, wd={wd}, dropout={drop}, hidden_dim={hd}\")\n",
    "        model, val_loss = train_and_evaluate_model(model_class, embedding_matrix, train_loader, val_loader, test_loader, lr=lr, weight_decay=wd, hidden_dim=hd, dropout=drop)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = {'lr': lr, 'weight_decay': wd, 'hidden_dim': hd, 'dropout': drop}\n",
    "            best_model_state = model.state_dict()\n",
    "    torch.save(best_model_state, f'best_{model_class.__name__}.pth')\n",
    "    with open(f'best_{model_class.__name__}_params.json', 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    return best_val_loss, best_params\n",
    "\n",
    "# Train and compare all models without Attention\n",
    "results = {}\n",
    "models = [SemanticClassifier, SemanticClassifierGRU, SemanticClassifierLSTM]\n",
    "\n",
    "for model_class in models:\n",
    "    print(f\"\\nTuning hyperparameters for {model_class.__name__}...\")\n",
    "    best_val_loss, best_params = tune_hyperparameters(model_class, embedding_matrix, train_loader, val_loader, test_loader)\n",
    "    results[model_class.__name__] = {'val_loss': best_val_loss, 'best_params': best_params}\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name}: Best Val Loss = {result['val_loss']:.4f}, Params = {result['best_params']}\")\n",
    "\n",
    "print(\"\\nEvaluating non-attention models on test set:\")\n",
    "for name, data in results.items():\n",
    "    best_params = data['best_params']\n",
    "    model_class = globals()[name]\n",
    "    model = model_class(embedding_matrix, hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout'], output_dim=1)\n",
    "    model.load_state_dict(torch.load(f'best_{name}.pth'))\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            output = model(batch_X)\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "            preds = (output > 0).int()\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(batch_y.int().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    data['test_acc'] = acc\n",
    "    data['test_prec'] = prec\n",
    "    data['test_rec'] = rec\n",
    "    data['test_f1'] = f1\n",
    "    print(f\"{name}: Acc {acc:.4f}, Prec {prec:.4f}, Rec {rec:.4f}, F1 {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compare all models with Attention\n",
    "results_att = {}\n",
    "models_att = [AttwSemanticClassifierGRU, AttwSemanticClassifierLSTM]\n",
    "\n",
    "for model_class in models_att:\n",
    "    print(f\"\\nTuning hyperparameters for {model_class.__name__}...\")\n",
    "    best_val_loss, best_params = tune_hyperparameters(model_class, embedding_matrix, train_loader, val_loader, test_loader)\n",
    "    results_att[model_class.__name__] = {'val_loss': best_val_loss, 'best_params': best_params}\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Results for Attention Models:\")\n",
    "for model_name, result in results_att.items():\n",
    "    print(f\"{model_name}: Best Val Loss = {result['val_loss']:.4f}, Params = {result['best_params']}\")\n",
    "\n",
    "print(\"\\nEvaluating attention models on test set:\")\n",
    "for name, data in results_att.items():\n",
    "    best_params = data['best_params']\n",
    "    model_class = globals()[name]\n",
    "    model = model_class(embedding_matrix, hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout'], output_dim=1)\n",
    "    model.load_state_dict(torch.load(f'best_{name}.pth'))\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            output, _ = model(batch_X)  # Unpack for attention\n",
    "            preds = (output > 0).int()\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(batch_y.int().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    data['test_acc'] = acc\n",
    "    data['test_prec'] = prec\n",
    "    data['test_rec'] = rec\n",
    "    data['test_f1'] = f1\n",
    "    print(f\"{name}: Acc {acc:.4f}, Prec {prec:.4f}, Rec {rec:.4f}, F1 {f1:.4f}\")\n",
    "\n",
    "# Find and save the overall best model\n",
    "all_results = {**results, **results_att}\n",
    "best_model_name = max(all_results, key=lambda x: all_results[x]['test_f1'])\n",
    "best_data = all_results[best_model_name]\n",
    "\n",
    "print(f\"\\nOverall best model: {best_model_name} with test F1: {best_data['test_f1']:.4f}\")\n",
    "\n",
    "# Save overall best model\n",
    "model_class = globals()[best_model_name]\n",
    "model = model_class(embedding_matrix, hidden_dim=best_data['best_params']['hidden_dim'], dropout=best_data['best_params']['dropout'], output_dim=1)\n",
    "model.load_state_dict(torch.load(f'best_{best_model_name}.pth'))\n",
    "torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Save hyperparameters\n",
    "with open('best_model_params.json', 'w') as f:\n",
    "    json.dump({'model_name': best_model_name, **best_data['best_params']}, f)\n",
    "\n",
    "print(\"Best model saved as 'best_model.pth' and params as 'best_model_params.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd36cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize for a single test sample\n",
    "# Use the best params for AttwSemanticClassifierLSTM\n",
    "best_params_lstm = results_att['AttwSemanticClassifierLSTM']['best_params']\n",
    "model = AttwSemanticClassifierLSTM(embedding_matrix, hidden_dim=best_params_lstm['hidden_dim'], dropout=best_params_lstm['dropout'], output_dim=1)\n",
    "model.load_state_dict(torch.load('best_AttwSemanticClassifierLSTM.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get a sample from test_loader\n",
    "sample_X, sample_y = next(iter(test_loader))\n",
    "sample_X = sample_X[:1]  # Take first batch item\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, att_weights = model(sample_X)\n",
    "    att_weights = att_weights.squeeze(0).cpu().numpy()  # Shape: (seq_len,)\n",
    "\n",
    "# Map back to words\n",
    "seq = sample_X.squeeze(0).cpu().numpy() \n",
    "words = [list(word_to_index.keys())[i] for i in seq if i not in [0, 2, 3]]  # Exclude <PAD>, <SOS>, <EOS>, but include <UNK> and content words\n",
    "# Attention weights correspond to positions 1 to 1+len(words)\n",
    "att_weights_trimmed = att_weights[1:1+len(words)]  # Trim to match words\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(words)), att_weights_trimmed, tick_label=words)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.title('Attention Weights for Sample Review')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
