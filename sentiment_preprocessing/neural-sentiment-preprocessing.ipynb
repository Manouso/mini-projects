{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45df197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7994107d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "(50000, 2)\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download necessary data\n",
    "import spacy.cli\n",
    "spacy.cli.download('en_core_web_sm') # Download English model\n",
    "\n",
    "# Load the IMDB dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the raw data\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])  # Load spacy without unnecessary components (for this project) for efficiency\n",
    "\n",
    "# Preprocessing function to clean reviews\n",
    "def preprocess_dataset(texts):\n",
    "    cleaned_texts = []\n",
    "    docs = list(nlp.pipe(texts, batch_size=1000))\n",
    "    for doc in docs:\n",
    "        # Collect lemmas for valid tokens in this doc\n",
    "        cleaned = [token.lemma_ for token in doc if not token.is_stop and (token.is_alpha or token.is_digit)]\n",
    "        # Join into a single string per review\n",
    "        cleaned_texts.append(' '.join(cleaned))\n",
    "    return cleaned_texts\n",
    "\n",
    "# Apply preprocessing to the reviews\n",
    "df['cleaned_review'] = preprocess_dataset(df['review'])\n",
    "print(df[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f97ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0  reviewer mention watch 1 Oz episode hook right...   \n",
      "1  wonderful little production br filming techniq...   \n",
      "2  think wonderful way spend time hot summer week...   \n",
      "3  basically family little boy Jake think zombie ...   \n",
      "4  Petter Mattei love Time money visually stunnin...   \n",
      "\n",
      "                                             bigrams  \n",
      "0  [(reviewer, mention), (mention, watch), (watch...  \n",
      "1  [(wonderful, little), (little, production), (p...  \n",
      "2  [(think, wonderful), (wonderful, way), (way, s...  \n",
      "3  [(basically, family), (family, little), (littl...  \n",
      "4  [(Petter, Mattei), (Mattei, love), (love, Time...  \n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams from cleaned text\n",
    "def generate_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Apply n-gram generation (bigrams)\n",
    "df['bigrams'] = df['cleaned_review'].apply(lambda x: generate_ngrams(x, 2))\n",
    "print(df[['cleaned_review', 'bigrams']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary with special tokens and indexing for neural network input\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "#Add special tokens for start and end of sequence <UKN> and <PAD> aren't needed here\n",
    "df['processed_review'] = df['cleaned_review'].apply(lambda x: f\"<SOS> {x} <EOS>\") # Add start and end tokens for sequence modeling\n",
    "\n",
    "#Initialize vectorizers with n-gram range and max features to limit vocabulary size for neural network input\n",
    "bow_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=1000) # Unigrams and bigrams, limit to top 1k features\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "\n",
    "# Sparse matrices for BoW and TF-IDF features\n",
    "bow_features = bow_vectorizer.fit_transform(df['processed_review'])\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['processed_review'])\n",
    "\n",
    "# Save vectorizers for later use in model training\n",
    "with open('vectorizers.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'bow_vectorizer': bow_vectorizer,\n",
    "        'tfidf_vectorizer': tfidf_vectorizer,\n",
    "        'bow_features': bow_features,\n",
    "        'tfidf_features': tfidf_features\n",
    "    }, f)\n",
    "\n",
    "print(\"Vectorizers and features saved to 'vectorizers.pkl'.\")\n",
    "print(f\"BoW feature shape: {bow_features.shape}\")\n",
    "print(f\"TF-IDF feature shape: {tfidf_features.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74993f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
