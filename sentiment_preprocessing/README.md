# IMDB Sentiment Text Preprocessing

This project demonstrates text preprocessing techniques for sentiment analysis on the IMDB movie reviews dataset. It includes two approaches: classical machine learning methods using Bag-of-Words (BoW) and TF-IDF vectorization, and neural network preparation with vocabulary building and sequence conversion.

## Features

### Classical Approach (`classical-sentiment-preprocessing.ipynb`)
- **Text Preprocessing**: Lemmatization, stopword removal, and punctuation filtering using spaCy.
- **Vectorization**: BoW and TF-IDF with n-gram support (unigrams and bigrams).
- **Model Training**: Logistic Regression and Naive Bayes classifiers with cross-validation.
- **Evaluation**: Classification reports and confusion matrices.

### Neural Approach (`neural-sentiment-preprocessing.ipynb`)
- **Text Preprocessing**: Lemmatization, stopword removal, and punctuation filtering using spaCy.
- **N-gram Generation**: Create bigrams from cleaned text.
- **Vocabulary Building**: Construct a vocabulary with special tokens (`<PAD>`, `<UNK>`, `<SOS>`, `<EOS>`) and word-to-index mappings.
- **Sequence Conversion**: Transform text into numerical sequences for neural model input.
- **Data Saving**: Export vocabulary and sequences to pickle files for reuse.

## Dataset

- **Source**: IMDB Dataset (50,000 movie reviews).
- **Columns**: `review` (text), `sentiment` (positive/negative).
- **File**: `IMDB Dataset.csv` (not included; downloaded from Kaggle).

## Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd sentiment_preprocessing
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Download the spaCy English model:
   ```bash
   python -m spacy download en_core_web_sm
   ```

## Usage

1. Place `IMDB Dataset.csv` in the project directory.

### Classical Approach
2. Open `classical-sentiment-preprocessing.ipynb` in Jupyter Notebook.
3. Run cells sequentially:
   - Load and preprocess data.
   - Vectorize text using BoW and TF-IDF.
   - Train and evaluate models (Logistic Regression, Naive Bayes).
   - Outputs include trained models and evaluation metrics.

### Neural Approach
2. Open `neural-sentiment-preprocessing.ipynb` in Jupyter Notebook.
3. Run cells sequentially:
   - Load and preprocess data.
   - Generate n-grams.
   - Build vocabulary.
   - Convert to sequences.
   - Save outputs (`vocabulary.pkl`, `sequences.pkl`).

### Example Output
- Vocabulary size: ~107k tokens (including specials).
- Sequences: Lists of integers representing words.
- Vectorized matrices for classical models.

## Dependencies

- pandas
- numpy
- scikit-learn
- spacy (with `en_core_web_sm` model)
- nltk

## Project Structure

```
sentiment_preprocessing/
├── classical-sentiment-preprocessing.ipynb  # Classical ML approach with BoW/TF-IDF
├── neural-sentiment-preprocessing.ipynb     # Neural network data preparation
├── IMDB Dataset.csv                         # Dataset (user-provided)
├── requirements.txt                         # Python dependencies
├── README.md                                # This file
├── vocabulary.pkl                           # Saved vocabulary (generated by neural notebook)
├── sequences.pkl                            # Saved sequences (generated by neural notebook)
└── vectorizers.pkl                          # Saved vectorizers (generated by classical notebook)
```
