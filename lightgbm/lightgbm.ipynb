{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec60cbe",
   "metadata": {},
   "source": [
    "# LightGBM Classification Project\n",
    "\n",
    "## Heart Disease Prediction using LightGBM\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline using **LightGBM (Light Gradient Boosting Machine)**, one of the most powerful and efficient gradient boosting frameworks.\n",
    "\n",
    "### What is LightGBM?\n",
    "\n",
    "LightGBM is a gradient boosting framework developed by Microsoft that uses tree-based learning algorithms. It's designed for:\n",
    "- **Speed**: Significantly faster training than traditional GBDT\n",
    "- **Efficiency**: Lower memory usage\n",
    "- **Accuracy**: High performance on various datasets\n",
    "- **Large datasets**: Optimized for datasets with millions of instances\n",
    "\n",
    "### Key Features:\n",
    "1. **Leaf-wise tree growth**: Grows trees by selecting leaves with maximum delta loss (vs level-wise)\n",
    "2. **Histogram-based algorithm**: Buckets continuous features into discrete bins\n",
    "3. **Gradient-based One-Side Sampling (GOSS)**: Keeps instances with large gradients\n",
    "4. **Exclusive Feature Bundling (EFB)**: Bundles mutually exclusive features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28cb04",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we import all necessary libraries for data manipulation, visualization, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Machine Learning - LightGBM\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    log_loss, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edfb87",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "**Dataset**: Heart Disease Dataset\n",
    "- **Source**: UCI Machine Learning Repository\n",
    "- **Objective**: Predict heart disease presence (binary classification)\n",
    "- **Features**: 11 clinical features including age, sex, chest pain type, blood pressure, cholesterol, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../datasets/heart.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initial Data Exploration\n",
    "print(\"=\"*80)\n",
    "print(\"Dataset Overview\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Basic Information\")\n",
    "print(\"=\"*80)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Statistical Summary\")\n",
    "print(\"=\"*80)\n",
    "print(df.describe().round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Target Variable Distribution\")\n",
    "print(\"=\"*80)\n",
    "target_counts = df['HeartDisease'].value_counts()\n",
    "print(f\"\\n{target_counts}\")\n",
    "print(f\"\\nClass Balance:\")\n",
    "print(f\"  - No Disease (0): {target_counts[0]} ({target_counts[0]/len(df)*100:.2f}%)\")\n",
    "print(f\"  - Disease (1): {target_counts[1]} ({target_counts[1]/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Missing Values Check\")\n",
    "print(\"=\"*80)\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dae3c9",
   "metadata": {},
   "source": [
    "### 2.1 Data Visualization\n",
    "\n",
    "Visualize the distribution of features and their relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "fig.suptitle('Heart Disease Dataset - Exploratory Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Target Distribution\n",
    "ax1 = axes[0, 0]\n",
    "target_counts = df['HeartDisease'].value_counts()\n",
    "colors = ['green', 'red']\n",
    "bars = ax1.bar(['No Disease', 'Disease'], target_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Count', fontsize=11)\n",
    "ax1.set_title('Target Variable Distribution', fontsize=12, fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}\\n({height/len(df)*100:.1f}%)',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. Age Distribution by Target\n",
    "ax2 = axes[0, 1]\n",
    "for target in [0, 1]:\n",
    "    subset = df[df['HeartDisease'] == target]['Age']\n",
    "    ax2.hist(subset, bins=20, alpha=0.6, label=f'Class {target}', edgecolor='black')\n",
    "ax2.set_xlabel('Age', fontsize=11)\n",
    "ax2.set_ylabel('Frequency', fontsize=11)\n",
    "ax2.set_title('Age Distribution by Heart Disease', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Chest Pain Type Distribution\n",
    "ax3 = axes[1, 0]\n",
    "chest_pain_counts = df.groupby(['ChestPainType', 'HeartDisease']).size().unstack()\n",
    "chest_pain_counts.plot(kind='bar', ax=ax3, color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Chest Pain Type', fontsize=11)\n",
    "ax3.set_ylabel('Count', fontsize=11)\n",
    "ax3.set_title('Chest Pain Type by Heart Disease', fontsize=12, fontweight='bold')\n",
    "ax3.legend(['No Disease', 'Disease'])\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Correlation Heatmap (numerical features only)\n",
    "ax4 = axes[1, 1]\n",
    "numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak', 'HeartDisease']\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, ax=ax4, cbar_kws={\"shrink\": 0.8})\n",
    "ax4.set_title('Feature Correlation Heatmap', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 5. MaxHR vs Age\n",
    "ax5 = axes[2, 0]\n",
    "for heart_disease in [0, 1]:\n",
    "    subset = df[df['HeartDisease'] == heart_disease]\n",
    "    ax5.scatter(subset['Age'], subset['MaxHR'], alpha=0.5, s=30,\n",
    "                label=f'Class {heart_disease}', edgecolors='black', linewidth=0.5)\n",
    "ax5.set_xlabel('Age', fontsize=11)\n",
    "ax5.set_ylabel('Max Heart Rate', fontsize=11)\n",
    "ax5.set_title('Max Heart Rate vs Age', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Sex Distribution\n",
    "ax6 = axes[2, 1]\n",
    "sex_counts = df.groupby(['Sex', 'HeartDisease']).size().unstack()\n",
    "sex_counts.plot(kind='bar', ax=ax6, color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "ax6.set_xlabel('Sex', fontsize=11)\n",
    "ax6.set_ylabel('Count', fontsize=11)\n",
    "ax6.set_title('Sex Distribution by Heart Disease', fontsize=12, fontweight='bold')\n",
    "ax6.legend(['No Disease', 'Disease'])\n",
    "ax6.tick_params(axis='x', rotation=0)\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec09e87",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing with Pipeline\n",
    "\n",
    "**Why Use Pipeline?**\n",
    "- **Reproducibility**: Ensures same transformations in train/test/production\n",
    "- **Prevention of Data Leakage**: Fit only on training data\n",
    "- **Cleaner Code**: Encapsulates all preprocessing steps\n",
    "- **Easy Deployment**: Save entire pipeline as single object\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. Handle invalid values (zeros in critical features)\n",
    "2. Encode categorical variables (label + one-hot encoding)\n",
    "3. Scale numerical features\n",
    "4. Train model\n",
    "\n",
    "**Note**: We'll create a custom transformer for handling zeros and use ColumnTransformer for different feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building preprocessing pipeline...\")\n",
    "\n",
    "# Prepare the raw data\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Identify feature types\n",
    "binary_categorical = ['Sex', 'ExerciseAngina']\n",
    "multi_categorical = ['ChestPainType', 'RestingECG', 'ST_Slope']\n",
    "numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "print(f\"\\nFeature types identified:\")\n",
    "print(f\"  - Binary categorical ({len(binary_categorical)}): {binary_categorical}\")\n",
    "print(f\"  - Multi-class categorical ({len(multi_categorical)}): {multi_categorical}\")\n",
    "print(f\"  - Numerical ({len(numerical_features)}): {numerical_features}\")\n",
    "\n",
    "# Function to impute zero values with median\n",
    "def impute_zeros(X):\n",
    "    X_copy = X.copy()\n",
    "    for col in ['Cholesterol', 'RestingBP']:\n",
    "        if col in X_copy.columns:\n",
    "            non_zero_values = X_copy[col][X_copy[col] > 0]\n",
    "            median_val = non_zero_values.median()\n",
    "            X_copy.loc[X_copy[col] == 0, col] = median_val\n",
    "    return X_copy\n",
    "\n",
    "# Function to encode binary categorical variables\n",
    "def encode_binary(X):\n",
    "    X_copy = X.copy()\n",
    "    for col in X_copy.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_copy[col] = le.fit_transform(X_copy[col])\n",
    "    return X_copy\n",
    "\n",
    "# Create preprocessing pipeline using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('binary_cat', Pipeline([\n",
    "            ('encoder', FunctionTransformer(encode_binary))\n",
    "        ]), binary_categorical),\n",
    "        \n",
    "        ('multi_cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ]), multi_categorical),\n",
    "        \n",
    "        ('num', Pipeline([\n",
    "            ('imputer', FunctionTransformer(impute_zeros)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec767956",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split & Validation Strategy\n",
    "\n",
    "**Strategy**: \n",
    "- Use **stratified split** to maintain class distribution\n",
    "- Split ratio: 80% training, 20% testing\n",
    "- Further split training into train/validation for hyperparameter tuning\n",
    "- Use **Stratified K-Fold Cross-Validation** for robust evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target from ORIGINAL data (before manual preprocessing)\n",
    "X = df_clean.drop('HeartDisease', axis=1)\n",
    "y = df_clean['HeartDisease']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal data (before preprocessing):\")\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Target vector shape: {y.shape}\")\n",
    "\n",
    "# Split into train and test sets (80-20 split, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\" Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Class Distribution (Stratified)\")\n",
    "print(\"-\"*80)\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  No Disease (0): {(y_train == 0).sum()} ({train_dist[0]:.1f}%)\")\n",
    "print(f\"  Disease (1): {(y_train == 1).sum()} ({train_dist[1]:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  No Disease (0): {(y_test == 0).sum()} ({test_dist[0]:.1f}%)\")\n",
    "print(f\"  Disease (1): {(y_test == 1).sum()} ({test_dist[1]:.1f}%)\")\n",
    "\n",
    "print(\"\\n Class distribution maintained across splits\")\n",
    "\n",
    "# Setup cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(f\"\\n Cross-validation: 5-Fold Stratified K-Fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7f182",
   "metadata": {},
   "source": [
    "## 5. Baseline LightGBM Model with Pipeline\n",
    "\n",
    "Create a complete Pipeline combining preprocessing and model training. This ensures:\n",
    "- **No data leakage**: Preprocessing fit only on training data\n",
    "- **Consistency**: Same transformations applied to train/test/production\n",
    "- **Simplicity**: One object handles everything\n",
    "- **Deployment-ready**: Save pipeline, not separate components\n",
    "\n",
    "### Pipeline Structure:\n",
    "```\n",
    "Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(...)),\n",
    "    ('classifier', LGBMClassifier(...))\n",
    "])\n",
    "```\n",
    "\n",
    "### LightGBM Key Parameters:\n",
    "- **n_estimators**: Number of boosting rounds (trees)\n",
    "- **learning_rate**: Step size shrinkage to prevent overfitting\n",
    "- **max_depth**: Maximum tree depth\n",
    "- **num_leaves**: Maximum number of leaves in one tree\n",
    "- **random_state**: For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline LightGBM Pipeline\")\n",
    "\n",
    "\n",
    "# Create complete pipeline with preprocessing + model\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
    "y_pred_proba_baseline = baseline_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline Model Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "precision = precision_score(y_test, y_pred_baseline)\n",
    "recall = recall_score(y_test, y_pred_baseline)\n",
    "f1 = f1_score(y_test, y_pred_baseline)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_baseline)\n",
    "mcc = matthews_corrcoef(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Score':<10}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"{'Accuracy':<20} {accuracy:.4f}\")\n",
    "print(f\"{'Precision':<20} {precision:.4f}\")\n",
    "print(f\"{'Recall':<20} {recall:.4f}\")\n",
    "print(f\"{'F1-Score':<20} {f1:.4f}\")\n",
    "print(f\"{'ROC-AUC':<20} {roc_auc:.4f}\")\n",
    "print(f\"{'Matthews Corr Coef':<20} {mcc:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Classification Report\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"-\"*80)\n",
    "print(cm_baseline)\n",
    "\n",
    "# Cross-validation score (on training data)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Cross-Validation Performance\")\n",
    "print(\"-\"*80)\n",
    "cv_scores = cross_val_score(baseline_pipeline, X_train, y_train, \n",
    "                            cv=cv_strategy, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"ROC-AUC CV Scores: {cv_scores}\")\n",
    "print(f\"Mean ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', square=True,\n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'],\n",
    "            cbar_kws={'label': 'Count'}, ax=ax)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_title('Baseline Pipeline - Confusion Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777ebed",
   "metadata": {},
   "source": [
    "## 6. Baseline Model Feature Importance\n",
    "\n",
    "Analyze which features are most important for the baseline model before adding engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d34932",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline model feature importance analysis...\")\n",
    "\n",
    "# Extract classifier from baseline pipeline\n",
    "lgb_baseline_classifier = baseline_pipeline.named_steps['classifier']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "preprocessor_baseline = baseline_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Build feature names\n",
    "baseline_feature_names = []\n",
    "baseline_feature_names.extend(binary_categorical)\n",
    "onehot_baseline = preprocessor_baseline.named_transformers_['multi_cat'].named_steps['onehot']\n",
    "onehot_baseline_names = onehot_baseline.get_feature_names_out(multi_categorical)\n",
    "baseline_feature_names.extend(onehot_baseline_names)\n",
    "baseline_feature_names.extend(numerical_features)\n",
    "\n",
    "# Get feature importance\n",
    "baseline_feature_importance = pd.DataFrame({\n",
    "    'Feature': baseline_feature_names,\n",
    "    'Importance': lgb_baseline_classifier.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nAll feature importances (baseline model):\")\n",
    "print(baseline_feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize all features\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, max(8, len(baseline_feature_importance) * 0.4)))\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(baseline_feature_importance)))\n",
    "ax.barh(range(len(baseline_feature_importance)), baseline_feature_importance['Importance'], color=colors, edgecolor='black')\n",
    "ax.set_yticks(range(len(baseline_feature_importance)))\n",
    "ax.set_yticklabels(baseline_feature_importance['Feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance Score')\n",
    "ax.set_title('All Features - Baseline Model', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Baseline feature importance analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27eee3",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Create new features based on domain knowledge to improve model performance. These engineered features will be used for hyperparameter tuning and the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating engineered features...\")\n",
    "\n",
    "# Function to create new features\n",
    "def create_engineered_features(X):\n",
    "    \"\"\"Create new features based on cardiovascular domain knowledge\"\"\"\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    # Heart rate metrics (using Age and MaxHR)\n",
    "    expected_max_hr = 220 - X['Age']\n",
    "    X_new['hr_reserve'] = X['MaxHR'] - expected_max_hr\n",
    "    X_new['hr_percentage'] = (X['MaxHR'] / expected_max_hr) * 100\n",
    "    X_new['hr_deficit'] = expected_max_hr - X['MaxHR']\n",
    "    \n",
    "    # Risk interactions (using Age, Oldpeak, Cholesterol)\n",
    "    X_new['oldpeak_age_interaction'] = X['Oldpeak'] * X['Age']\n",
    "    X_new['age_chol_interaction'] = X['Age'] * X['Cholesterol'] / 1000\n",
    "    \n",
    "    # Combined categorical feature (using ChestPainType and ExerciseAngina)\n",
    "    X_new['pain_exercise_combo'] = X['ChestPainType'].astype(str) + '_' + X['ExerciseAngina'].astype(str)\n",
    "\n",
    "    # Now drop the original features that were used to create new ones\n",
    "    X_new = X_new.drop(columns=['Age', 'MaxHR', 'Oldpeak', 'ChestPainType', 'ExerciseAngina'])\n",
    "\n",
    "    '''# Drop features with low importance based on baseline analysis\n",
    "    X_new = X_new.drop(columns=['FastingBS', 'RestingECG'])'''\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# Apply to both train and test sets\n",
    "X_train_eng = create_engineered_features(X_train)\n",
    "X_test_eng = create_engineered_features(X_test)\n",
    "\n",
    "print(f\"\\nOriginal features: {X_train.shape[1]}\")\n",
    "print(f\"Engineered features: {X_train_eng.shape[1]}\")\n",
    "print(f\"Features removed: {X_train.shape[1] - X_train_eng.shape[1]}\")\n",
    "\n",
    "print(f\"\\nRemaining features after engineering:\")\n",
    "for feat in sorted(X_train_eng.columns):\n",
    "    print(f\"  • {feat}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9f51f",
   "metadata": {},
   "source": [
    "## 8. Preprocessing Pipeline for Engineered Features\n",
    "\n",
    "Update preprocessing to handle the new engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building preprocessing pipeline for engineered features...\")\n",
    "\n",
    "binary_categorical_eng = ['Sex']\n",
    "multi_categorical_eng = ['ST_Slope', 'RestingECG', 'pain_exercise_combo']\n",
    "numerical_features_eng = ['RestingBP', 'Cholesterol', 'FastingBS', \n",
    "                         'hr_reserve', 'hr_percentage', 'hr_deficit', \n",
    "                         'oldpeak_age_interaction', 'age_chol_interaction']\n",
    "\n",
    "print(f\"\\nEngineered feature types:\")\n",
    "print(f\"  • Binary categorical: {binary_categorical_eng}\")\n",
    "print(f\"  • Multi-class categorical: {multi_categorical_eng}\")\n",
    "print(f\"  • Numerical: {numerical_features_eng}\")\n",
    "\n",
    "# Create new preprocessor for engineered features\n",
    "preprocessor_eng = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('binary_cat', Pipeline([\n",
    "            ('encoder', FunctionTransformer(encode_binary))\n",
    "        ]), binary_categorical_eng),\n",
    "        \n",
    "        ('multi_cat', Pipeline([\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ]), multi_categorical_eng),\n",
    "        \n",
    "        ('num', Pipeline([\n",
    "            ('imputer', FunctionTransformer(impute_zeros)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features_eng)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing pipeline ready for engineered features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb52ae2",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning with Pipeline\n",
    "\n",
    "Use **RandomizedSearchCV** on the entire pipeline to find optimal hyperparameters.\n",
    "\n",
    "### Pipeline Hyperparameter Syntax:\n",
    "When using Pipeline, parameter names must include the step name:\n",
    "- `classifier__n_estimators` (not just `n_estimators`)\n",
    "- `classifier__learning_rate` (not just `learning_rate`)\n",
    "\n",
    "### Tuning Strategy:\n",
    "- RandomizedSearchCV for efficient exploration\n",
    "- 50 random combinations from parameter space\n",
    "- 5-Fold Stratified Cross-Validation\n",
    "- Optimize for ROC-AUC (good for binary classification)\n",
    "- Tune only classifier parameters (preprocessing is fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Hyperparameter Tuning With Pipeline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define parameter grid for the CLASSIFIER in the pipeline\n",
    "# Note: Use 'classifier__' prefix for pipeline parameters\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 500],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 5, 7, 10, -1],\n",
    "    'classifier__num_leaves': [15, 31, 63, 127],\n",
    "    'classifier__min_child_samples': [5, 10, 20, 30],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'classifier__reg_lambda': [0, 0.01, 0.1, 1],\n",
    "    'classifier__min_split_gain': [0.0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "print(f\"\\n Parameter Search Space (Pipeline Syntax):\")\n",
    "total_combinations = 1\n",
    "for param, values in param_distributions.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\n Total possible combinations: {total_combinations:,}\")\n",
    "\n",
    "# Create pipeline for tuning (using engineered features)\n",
    "tuning_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_eng),\n",
    "    ('classifier', LGBMClassifier(random_state=42, verbose=-1))\n",
    "])\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "n_iter = 20\n",
    "print(f\"\\n  Sampling {n_iter} random combinations...\")\n",
    "print(f\" Cross-validation: {cv_strategy.n_splits}-Fold Stratified\")\n",
    "print(f\" Scoring metric: ROC-AUC\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=tuning_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iter,\n",
    "    cv=cv_strategy,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Starting hyperparameter tuning...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "time_start = time.time()\n",
    "random_search.fit(X_train_eng, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "print(f\"\\n Tuning completed in {time_end - time_start:.2f} seconds!\")\n",
    "\n",
    "# Best parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Best Hyperparameters\")\n",
    "print(\"=\"*80)\n",
    "best_params = random_search.best_params_\n",
    "for param, value in sorted(best_params.items()):\n",
    "    # Remove 'classifier__' prefix for display\n",
    "    display_param = param.replace('classifier__', '')\n",
    "    print(f\"  {display_param:<20}: {value}\")\n",
    "\n",
    "print(f\"\\n Best CV ROC-AUC Score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Store the best pipeline\n",
    "optimized_pipeline = random_search.best_estimator_\n",
    "\n",
    "# Display top 10 parameter combinations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 10 Parameter Combinations\")\n",
    "print(\"=\"*80)\n",
    "results_df = pd.DataFrame(random_search.cv_results_)\n",
    "top_10 = results_df.nsmallest(10, 'rank_test_score')[\n",
    "    ['rank_test_score', 'mean_test_score', 'std_test_score', 'params']\n",
    "]\n",
    "\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"\\nRank {int(row['rank_test_score'])}:\")\n",
    "    print(f\" Mean ROC-AUC: {row['mean_test_score']:.4f} (+/- {row['std_test_score']:.4f})\")\n",
    "    # Clean up parameter display\n",
    "    params_clean = {k.replace('classifier__', ''): v for k, v in row['params'].items()}\n",
    "    print(f\" Parameters: {params_clean}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417979e",
   "metadata": {},
   "source": [
    "## 10. Optimized Model Evaluation\n",
    "\n",
    "Evaluate the tuned model and compare with the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with optimized pipeline\n",
    "y_pred_optimized = optimized_pipeline.predict(X_test_eng)\n",
    "y_pred_proba_optimized = optimized_pipeline.predict_proba(X_test_eng)[:, 1]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Optimized Pipeline Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_opt = accuracy_score(y_test, y_pred_optimized)\n",
    "precision_opt = precision_score(y_test, y_pred_optimized)\n",
    "recall_opt = recall_score(y_test, y_pred_optimized)\n",
    "f1_opt = f1_score(y_test, y_pred_optimized)\n",
    "roc_auc_opt = roc_auc_score(y_test, y_pred_proba_optimized)\n",
    "mcc_opt = matthews_corrcoef(y_test, y_pred_optimized)\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Score':<10}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"{'Accuracy':<20} {accuracy_opt:.4f}\")\n",
    "print(f\"{'Precision':<20} {precision_opt:.4f}\")\n",
    "print(f\"{'Recall':<20} {recall_opt:.4f}\")\n",
    "print(f\"{'F1-Score':<20} {f1_opt:.4f}\")\n",
    "print(f\"{'ROC-AUC':<20} {roc_auc_opt:.4f}\")\n",
    "print(f\"{'Matthews Corr Coef':<20} {mcc_opt:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Classification Report\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_pred_optimized, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"-\"*80)\n",
    "print(cm_optimized)\n",
    "\n",
    "# Model Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline vs Optimized Pipeline Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'MCC'],\n",
    "    'Baseline': [accuracy, precision, recall, f1, roc_auc, mcc],\n",
    "    'Optimized': [accuracy_opt, precision_opt, recall_opt, f1_opt, roc_auc_opt, mcc_opt],\n",
    "    'Improvement': [\n",
    "        accuracy_opt - accuracy,\n",
    "        precision_opt - precision,\n",
    "        recall_opt - recall,\n",
    "        f1_opt - f1,\n",
    "        roc_auc_opt - roc_auc,\n",
    "        mcc_opt - mcc\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline'] * 100).round(2)\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualization: Side-by-side comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Baseline vs Optimized Pipeline Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Confusion Matrices\n",
    "ax1 = axes[0, 0]\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', square=True,\n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'],\n",
    "            cbar=False, ax=ax1)\n",
    "ax1.set_ylabel('Actual', fontsize=11)\n",
    "ax1.set_xlabel('Predicted', fontsize=11)\n",
    "ax1.set_title('Baseline Pipeline - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Greens', square=True,\n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'],\n",
    "            cbar=False, ax=ax2)\n",
    "ax2.set_ylabel('Actual', fontsize=11)\n",
    "ax2.set_xlabel('Predicted', fontsize=11)\n",
    "ax2.set_title('Optimized Pipeline - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. ROC Curves\n",
    "ax3 = axes[1, 0]\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_pred_proba_baseline)\n",
    "fpr_opt, tpr_opt, _ = roc_curve(y_test, y_pred_proba_optimized)\n",
    "\n",
    "ax3.plot(fpr_base, tpr_base, label=f'Baseline (AUC = {roc_auc:.4f})', linewidth=2, color='blue')\n",
    "ax3.plot(fpr_opt, tpr_opt, label=f'Optimized (AUC = {roc_auc_opt:.4f})', linewidth=2, color='green')\n",
    "ax3.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "ax3.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax3.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax3.set_title('ROC Curves Comparison', fontsize=12, fontweight='bold')\n",
    "ax3.legend(loc='lower right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Metrics Comparison Bar Chart\n",
    "ax4 = axes[1, 1]\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "baseline_scores = [accuracy, precision, recall, f1, roc_auc]\n",
    "optimized_scores = [accuracy_opt, precision_opt, recall_opt, f1_opt, roc_auc_opt]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "bars1 = ax4.bar(x - width/2, baseline_scores, width, label='Baseline', color='skyblue', edgecolor='black')\n",
    "bars2 = ax4.bar(x + width/2, optimized_scores, width, label='Optimized', color='lightgreen', edgecolor='black')\n",
    "\n",
    "ax4.set_xlabel('Metrics', fontsize=11)\n",
    "ax4.set_ylabel('Score', fontsize=11)\n",
    "ax4.set_title('Performance Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.set_ylim([0, 1.1])\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8a372",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis (From Pipeline)\n",
    "\n",
    "Extract feature importance from the trained pipeline's classifier component.\n",
    "\n",
    "**Challenge with Pipelines:**\n",
    "- Feature names change after preprocessing (one-hot encoding creates new features)\n",
    "- Need to map transformed feature names back to original features\n",
    "\n",
    "**Solution:**\n",
    "- Access the classifier: `pipeline.named_steps['classifier']`\n",
    "- Get transformed feature names from preprocessor\n",
    "- Match importance scores to feature names\n",
    "\n",
    "LightGBM provides feature importance based on:\n",
    "- **split**: Number of times feature is used in a split\n",
    "- **gain**: Total information gain when splitting on the feature (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Feature Importance Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract the classifier from the pipeline\n",
    "lgb_classifier = optimized_pipeline.named_steps['classifier']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "# This requires getting the transformed feature names from the ColumnTransformer\n",
    "preprocessor_fitted = optimized_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Build feature names from the ColumnTransformer (with engineered features)\n",
    "feature_names = []\n",
    "\n",
    "# Binary categorical features (label encoded) - keep original names\n",
    "feature_names.extend(binary_categorical_eng)\n",
    "\n",
    "# Multi-categorical features (one-hot encoded) - get new names\n",
    "onehot_encoder = preprocessor_fitted.named_transformers_['multi_cat'].named_steps['onehot']\n",
    "onehot_feature_names = onehot_encoder.get_feature_names_out(multi_categorical_eng)\n",
    "feature_names.extend(onehot_feature_names)\n",
    "\n",
    "# Numerical features - keep original names\n",
    "feature_names.extend(numerical_features_eng)\n",
    "\n",
    "print(f\"\\n Transformed Features: {len(feature_names)} total\")\n",
    "print(f\"  - Binary categorical: {len(binary_categorical_eng)}\")\n",
    "print(f\"  - One-hot encoded: {len(onehot_feature_names)}\")\n",
    "print(f\"  - Numerical: {len(numerical_features_eng)}\")\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': lgb_classifier.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"All Feature Importances (Optimized Model)\")\n",
    "print(\"-\"*80)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, max(10, len(feature_importance) * 0.3)))\n",
    "fig.suptitle('Feature Importance Analysis (Optimized Model)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# All features - horizontal bar chart\n",
    "ax1 = axes[0]\n",
    "colors_gradient = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))\n",
    "ax1.barh(range(len(feature_importance)), feature_importance['Importance'], color=colors_gradient, edgecolor='black')\n",
    "ax1.set_yticks(range(len(feature_importance)))\n",
    "ax1.set_yticklabels(feature_importance['Feature'])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Importance Score', fontsize=12)\n",
    "ax1.set_title('All Features Importance', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(feature_importance.iterrows()):\n",
    "    ax1.text(row['Importance'], i, f' {row[\"Importance\"]:.0f}', \n",
    "             va='center', fontsize=8)\n",
    "\n",
    "# Feature importance distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(feature_importance['Importance'], bins=20, color='steelblue', \n",
    "         alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Importance Score', fontsize=12)\n",
    "ax2.set_ylabel('Number of Features', fontsize=12)\n",
    "ax2.set_title('Distribution of Feature Importance', fontsize=13, fontweight='bold')\n",
    "ax2.axvline(feature_importance['Importance'].mean(), color='red', \n",
    "            linestyle='--', linewidth=2, label=f'Mean: {feature_importance[\"Importance\"].mean():.1f}')\n",
    "ax2.axvline(feature_importance['Importance'].median(), color='green', \n",
    "            linestyle='--', linewidth=2, label=f'Median: {feature_importance[\"Importance\"].median():.1f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36f209",
   "metadata": {},
   "source": [
    "## 12. Model Interpretation & Predictions (Using Pipeline)\n",
    "\n",
    "Demonstrate how to use the trained pipeline for making predictions on new data.\n",
    "\n",
    "**Pipeline Advantages for Predictions:**\n",
    "- Pass raw, unprocessed data directly to pipeline\n",
    "- Pipeline handles all preprocessing automatically\n",
    "- Consistent transformations guaranteed\n",
    "- Single `.predict()` call handles everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Predictions - Sample Cases\")\n",
    "\n",
    "# Select random samples from test set (using engineered data)\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(X_test.index, size=5, replace=False)\n",
    "sample_data_raw = X_test_eng.loc[sample_indices]  # Engineered data\n",
    "sample_actual = y_test.loc[sample_indices]\n",
    "\n",
    "# Make predictions using the PIPELINE (handles preprocessing automatically)\n",
    "sample_predictions = optimized_pipeline.predict(sample_data_raw)\n",
    "sample_probabilities = optimized_pipeline.predict_proba(sample_data_raw)\n",
    "\n",
    "print(\"Prediction Examples\")\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {i+1} (Index: {idx})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get original data (before engineering)\n",
    "    original_row = X_test.loc[idx]\n",
    "    \n",
    "    print(\"\\nPatient Information (Original Input):\")\n",
    "    print(f\"  Age: {original_row['Age']} years\")\n",
    "    print(f\"  Sex: {original_row['Sex']}\")\n",
    "    print(f\"  Chest Pain Type: {original_row['ChestPainType']}\")\n",
    "    print(f\"  Resting BP: {original_row['RestingBP']} mm Hg\")\n",
    "    print(f\"  Cholesterol: {original_row['Cholesterol']} mg/dl\")\n",
    "    print(f\"  Fasting Blood Sugar > 120: {'Yes' if original_row['FastingBS'] else 'No'}\")\n",
    "    print(f\"  Resting ECG: {original_row['RestingECG']}\")\n",
    "    print(f\"  Max Heart Rate: {original_row['MaxHR']} bpm\")\n",
    "    print(f\"  Exercise Angina: {original_row['ExerciseAngina']}\")\n",
    "    print(f\"  Oldpeak: {original_row['Oldpeak']}\")\n",
    "    print(f\"  ST Slope: {original_row['ST_Slope']}\")\n",
    "    \n",
    "    print(f\"\\nPipeline Prediction:\")\n",
    "    print(f\"  Actual: {'Heart Disease' if sample_actual.iloc[i] else 'No Heart Disease'}\")\n",
    "    print(f\"  Predicted: {'Heart Disease' if sample_predictions[i] else 'No Heart Disease'}\")\n",
    "    print(f\"  Confidence (No Disease): {sample_probabilities[i][0]:.2%}\")\n",
    "    print(f\"  Confidence (Disease): {sample_probabilities[i][1]:.2%}\")\n",
    "    print(f\"  Status: {'✓ CORRECT' if sample_predictions[i] == sample_actual.iloc[i] else '✗ INCORRECT'}\")\n",
    "\n",
    "# Probability distribution visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Prediction Confidence Distribution\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all test predictions using pipeline (with engineered features)\n",
    "all_proba = optimized_pipeline.predict_proba(X_test_eng)[:, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Model Prediction Confidence Analysis (Pipeline)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Probability distribution by actual class\n",
    "ax1 = axes[0]\n",
    "for label in [0, 1]:\n",
    "    mask = y_test == label\n",
    "    proba_subset = all_proba[mask]\n",
    "    ax1.hist(proba_subset, bins=30, alpha=0.6, \n",
    "             label=f'Actual: {\"Disease\" if label else \"No Disease\"}',\n",
    "             edgecolor='black')\n",
    "ax1.set_xlabel('Predicted Probability of Disease', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Prediction Probability Distribution by Actual Class', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.axvline(0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Confidence levels\n",
    "ax2 = axes[1]\n",
    "confidence = np.maximum(all_proba, 1 - all_proba)\n",
    "confidence_bins = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "confidence_labels = ['50-60%', '60-70%', '70-80%', '80-90%', '90-100%']\n",
    "confidence_counts = [np.sum((confidence >= confidence_bins[i]) & (confidence < confidence_bins[i+1])) \n",
    "                     for i in range(len(confidence_bins)-1)]\n",
    "\n",
    "colors_conf = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(confidence_labels)))\n",
    "bars = ax2.bar(confidence_labels, confidence_counts, color=colors_conf, edgecolor='black')\n",
    "ax2.set_xlabel('Confidence Level', fontsize=12)\n",
    "ax2.set_ylabel('Number of Predictions', fontsize=12)\n",
    "ax2.set_title('Prediction Confidence Distribution', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}\\n({height/len(all_proba)*100:.1f}%)',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
