{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b02479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for neural network using numpy\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58016e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network implementation using NumPy\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size, learning_rate=0.01):\n",
    "\n",
    "        \"\"\"Initialization of a single neuron with weights and bias.\"\"\"\n",
    "\n",
    "        # Initialize weights with He initialization for better convergence\n",
    "        self.weights = np.random.randn(input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Placeholders for storing intermediate values\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_derivative(self, x):\n",
    "        \"\"\"Derivative of the tanh function.\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of the ReLU function.\"\"\"\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        \"\"\"Leaky ReLU activation function.\"\"\"\n",
    "        return np.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def leaky_relu_derivative(self, x, alpha=0.01):\n",
    "        \"\"\"Derivative of the Leaky ReLU function.\"\"\"\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    # Forward Propagation\n",
    "    def forward(self, X, activation='relu'):\n",
    "        \"\"\"Forward pass through the neuron.\"\"\"\n",
    "\n",
    "        # Linear Input\n",
    "        self.input = X\n",
    "\n",
    "        # Linear combination output\n",
    "        self.linear_output = np.dot(self.input, self.weights) + self.bias\n",
    "\n",
    "        # Activation\n",
    "        if activation == 'sigmoid':\n",
    "            self.activated_output = self.sigmoid(self.linear_output)\n",
    "        elif activation == 'tanh':\n",
    "            self.activated_output = self.tanh(self.linear_output)\n",
    "        elif activation == 'relu':\n",
    "            self.activated_output = self.relu(self.linear_output)\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activated_output = self.leaky_relu(self.linear_output)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function {activation}\")\n",
    "        \n",
    "        return self.activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neuron instance\n",
    "neuron = Neuron(input_size=3, learning_rate=0.01)\n",
    "\n",
    "# Create sample input data for plotting\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "\n",
    "# Calculate activations and derivatives\n",
    "\n",
    "alpha = 0.1  # Increased from default 0.01 for better visualization\n",
    "\n",
    "sigmoid_out = neuron.sigmoid(x)\n",
    "tanh_out = neuron.tanh(x)\n",
    "relu_out = neuron.relu(x)\n",
    "leaky_relu_out = neuron.leaky_relu(x, alpha=alpha)\n",
    "\n",
    "sigmoid_deriv = neuron.sigmoid_derivative(x)\n",
    "tanh_deriv = neuron.tanh_derivative(x)\n",
    "relu_deriv = neuron.relu_derivative(x)\n",
    "leaky_relu_deriv = neuron.leaky_relu_derivative(x, alpha=alpha)\n",
    "\n",
    "# Set up professional plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Colors for consistency\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "activation_names = ['Sigmoid', 'Tanh', 'ReLU', f'Leaky ReLU (α={alpha})']\n",
    "activations = [sigmoid_out, tanh_out, relu_out, leaky_relu_out]\n",
    "derivatives = [sigmoid_deriv, tanh_deriv, relu_deriv, leaky_relu_deriv]\n",
    "\n",
    "# Plot activations (top row)\n",
    "for i, (name, activation, color) in enumerate(zip(activation_names, activations, colors)):\n",
    "    axes[0, i].plot(x, activation, color=color, linewidth=3, alpha=0.8)\n",
    "    axes[0, i].set_title(f'{name} Activation', fontsize=14, fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Input (z)')\n",
    "    axes[0, i].set_ylabel('Output')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    axes[0, i].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[0, i].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot derivatives (bottom row)\n",
    "for i, (name, derivative, color) in enumerate(zip(activation_names, derivatives, colors)):\n",
    "    axes[1, i].plot(x, derivative, color=color, linewidth=3, alpha=0.8)\n",
    "    axes[1, i].set_title(f'{name.replace(f\" (α={alpha})\", \"\")} Derivative', fontsize=14, fontweight='bold')\n",
    "    axes[1, i].set_xlabel('Input (z)')\n",
    "    axes[1, i].set_ylabel('Derivative')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "    axes[1, i].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1, i].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911969ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer neural network implementation\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Create 2D dataset with make_circles\n",
    "X, y = make_circles(n_samples=100, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.title('2D Dataset from make_circles', fontsize=16)\n",
    "plt.xlabel('Feature 1', fontsize=14)\n",
    "plt.ylabel('Feature 2', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7230f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer neural network implementation\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self, input_size=2, hidden_size=4, output_size=1, learning_rate=0.01, random_state=42):\n",
    "        \"\"\"Initialize the neural network with random weights and biases.\"\"\"\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Hidden layer weights and biases (He initialization)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Output layer weights and biases\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Store intermediate values for backpropagation\n",
    "        self.Z1 = None\n",
    "        self.A1 = None\n",
    "        self.Z2 = None\n",
    "        self.A2 = None\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        \"\"\"Derivative of ReLU.\"\"\"\n",
    "        return np.where(Z > 0, 1, 0)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"Backward pass to compute gradients and update parameters.\"\"\"\n",
    "        \n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = y_pred - y_true.reshape(-1, 1)  # ∂L/∂Z2\n",
    "        dW2 = (1/m) * np.dot(self.A1.T, dZ2)  # ∂L/∂W2\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)  # ∂L/∂b2\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(dZ2, self.W2.T)  # ∂L/∂A1\n",
    "        dZ1 = dA1 * self.relu_derivative(self.Z1)  # ∂L/∂Z1\n",
    "        dW1 = (1/m) * np.dot(X.T, dZ1)  # ∂L/∂W1\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)  # ∂L/∂b1\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions (0 or 1) based on threshold 0.5.\"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities > 0.5).astype(int).flatten()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return prediction probabilities.\"\"\"\n",
    "        return self.forward(X).flatten()\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, y_pred)\n",
    "            \n",
    "            # Print progress every 100 epochs\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the neural network\n",
    "\n",
    "nn = TwoLayerNN(input_size=2, hidden_size=16, output_size=1, learning_rate=0.01, random_state=42)\n",
    "\n",
    "loss_history = nn.train(X, y, epochs=1000)\n",
    "\n",
    "# Plot training loss over epochs\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, linewidth=2)\n",
    "plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Binary Cross-Entropy Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Get predictions\n",
    "predictions = nn.predict(X)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y) * 100\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagram 1: Model Performance (Accuracy + Decision Boundary)\n",
    "\n",
    "predictions = nn.predict(X)\n",
    "accuracy = np.mean(predictions == y) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Decision boundary plot\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "\n",
    "Z = nn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=50)\n",
    "plt.title(f'Model Performance: Decision Boundary (Accuracy: {accuracy:.2f}%)', fontsize=16)\n",
    "plt.xlabel('Feature 1', fontsize=14)\n",
    "plt.ylabel('Feature 2', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d00cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagram 2: Model Confidence (Prediction Probabilities Histogram)\n",
    "\n",
    "probabilities = nn.predict_proba(X)\n",
    "\n",
    "# Histogram of probabilities\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(probabilities, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "plt.title('Model Confidence: Distribution of Prediction Probabilities', fontsize=16)\n",
    "plt.xlabel('Predicted Probability', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Confidence metrics\n",
    "\n",
    "confident_predictions = np.sum((probabilities > 0.8) | (probabilities < 0.2))\n",
    "total_predictions = len(probabilities)\n",
    "confidence_rate = confident_predictions / total_predictions * 100\n",
    "print(f\"Confident Predictions (>0.8 or <0.2): {confidence_rate:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
