{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fb74ff",
   "metadata": {},
   "source": [
    "# EfficientNet-B1 Transfer Learning on CIFAR-10\n",
    "\n",
    "This notebook demonstrates advanced transfer learning using EfficientNet-B1 with hyperparameter optimization and fine-tuning for CIFAR-10 classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7699dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from torch.amp import autocast\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a56ee",
   "metadata": {},
   "source": [
    "## 1. Hyperparameter Configuration\n",
    "\n",
    "Define the hyperparameter search space for random search optimization of the classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b97176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space for classifier optimization\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.005, 0.001, 0.0005],\n",
    "    'weight_decay': [0, 1e-4, 5e-4, 1e-2, 5e-2],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'optimizer': ['adamw', 'sgd'],\n",
    "    'scheduler': ['cosine', 'step', 'exponential'],\n",
    "    'classifier_type': ['simple','deep']\n",
    "    \n",
    "}\n",
    "\n",
    "num_trials = 20  # Increased for better optimization\n",
    "batch_size = 128  # Fixed batch size for consistency\n",
    "num_epochs = 20  # Final training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec05a1",
   "metadata": {},
   "source": [
    "## 2. Model Architecture\n",
    "\n",
    "Define the EfficientNet-B1 model with customizable classifier heads for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a67fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model with frozen backbone and tunable classifier\n",
    "def create_model_with_params(dropout_rate=0.0, freeze_backbone=True, classifier_type='deep', use_qat=False):\n",
    "    model = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    if freeze_backbone:\n",
    "        # Freeze all layers except classifier\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Replace classifier with tunable head\n",
    "    if classifier_type == 'simple':\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(model.classifier[1].in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    elif classifier_type == 'deep':\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Linear(model.classifier[1].in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    # Unfreeze classifier parameters\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    if use_qat:\n",
    "        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        torch.prepare_qat(model, inplace=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to create optimizer and scheduler\n",
    "def create_optimizer_scheduler(model, optimizer_name, lr, weight_decay, scheduler_name, steps_per_epoch=None):\n",
    "    \n",
    "    if optimizer_name == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    if scheduler_name == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    elif scheduler_name == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    elif scheduler_name == 'exponential':\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "# Hyperparameter tuning function for classifier optimization\n",
    "def hyperparameter_tuning(train_dataset, val_dataset, param_distributions, num_trials, device):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    param_list = list(ParameterSampler(param_distributions, n_iter=num_trials, random_state=42))\n",
    "\n",
    "    for i, params in enumerate(param_list):\n",
    "        print(f\"\\nTrial {i+1}/{num_trials}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        # Create model with frozen backbone\n",
    "        model = create_model_with_params(params['dropout_rate'], freeze_backbone=True, classifier_type=params['classifier_type'])\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Create optimizer and scheduler (only for classifier parameters)\n",
    "        optimizer, scheduler = create_optimizer_scheduler(\n",
    "            model, params['optimizer'], params['learning_rate'],\n",
    "            params['weight_decay'], params['scheduler'], steps_per_epoch=len(train_loader)\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Quick training for hyperparameter evaluation (3 epochs)\n",
    "        train_losses, train_accuracies, val_losses, val_accuracies, _ = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=3, device=device, use_amp=True\n",
    "        )\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_accuracy, _ = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'train_losses': train_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies\n",
    "        })\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_accuracy, results\n",
    "\n",
    "# Training function with validation and learning rate monitoring\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, fine_tune_backbone=False, patience=10, use_amp=False):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    learning_rates = []  # Track learning rates\n",
    "\n",
    "    scaler = torch.amp.GradScaler() if use_amp and device.type == 'cuda' else None\n",
    "\n",
    "    best_val_acc = -float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast('cuda', enabled=use_amp):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            if use_amp and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = 100. * correct / total\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "        # Track learning rates (for monitoring fine-tuning)\n",
    "        if fine_tune_backbone and hasattr(optimizer, 'param_groups'):\n",
    "            current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "            learning_rates.append(current_lrs)\n",
    "            if epoch == 0 or epoch == num_epochs - 1:\n",
    "                # print learning rates of backbone and classifier\n",
    "                print(f\"Epoch {epoch+1} LRs: Backbone={current_lrs[0]:.6f}, Classifier={current_lrs[1]:.6f}\")\n",
    "        else:\n",
    "            learning_rates.append([optimizer.param_groups[0]['lr']])\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                with autocast('cuda', enabled=use_amp):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_loader)\n",
    "        epoch_val_acc = 100. * val_correct / val_total\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "\n",
    "        # Early stopping check\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies, learning_rates\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    inference_times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            inference_time = time.time() - start_time\n",
    "            inference_times.append(inference_time / inputs.size(0))  # Per sample\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    avg_inference_time = np.mean(inference_times) * 1000  # Convert to milliseconds\n",
    "\n",
    "    return accuracy, avg_inference_time\n",
    "\n",
    "# Function to count trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7cd9b",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning\n",
    "\n",
    "Perform random search hyperparameter optimization on the classifier head with frozen backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data transforms with advanced augmentation and proper resizing for EfficientNet\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize to 256 first\n",
    "    transforms.RandomCrop(224),  # Then crop to 224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # CIFAR-10 normalization\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Class names\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Perform hyperparameter tuning on classifier\n",
    "print(\"Starting hyperparameter tuning for classifier optimization...\")\n",
    "best_params, best_accuracy, tuning_results = hyperparameter_tuning(\n",
    "    train_dataset, val_dataset, param_distributions, num_trials, device\n",
    ")\n",
    "\n",
    "print(f\"\\nBest hyperparameters found: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "# Train final model with best parameters (full training)\n",
    "print(\"\\nTraining final model with best hyperparameters...\")\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "final_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "final_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Option to fine-tune backbone with reduced learning rate\n",
    "fine_tune_backbone = True  # Set to True to fine-tune backbone with reduced LR\n",
    "backbone_lr_multiplier = 0.1  # Backbone learning rate = classifier_lr * multiplier\n",
    "\n",
    "if fine_tune_backbone:\n",
    "    final_model = create_model_with_params(best_params['dropout_rate'], freeze_backbone=False)\n",
    "\n",
    "    # Use different learning rates for backbone and classifier\n",
    "    backbone_lr = best_params['learning_rate'] * backbone_lr_multiplier\n",
    "    classifier_lr = best_params['learning_rate']\n",
    "\n",
    "    backbone_params = [p for name, p in final_model.named_parameters() if 'classifier' not in name]\n",
    "    classifier_params = list(final_model.classifier.parameters())\n",
    "\n",
    "    if best_params['optimizer'] == 'adamw':\n",
    "        final_optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': backbone_lr, 'weight_decay': best_params['weight_decay']},\n",
    "            {'params': classifier_params, 'lr': classifier_lr, 'weight_decay': best_params['weight_decay']}\n",
    "        ])\n",
    "    else:  # sgd\n",
    "        final_optimizer = optim.SGD([\n",
    "            {'params': backbone_params, 'lr': backbone_lr, 'momentum': 0.9, 'weight_decay': best_params['weight_decay']},\n",
    "            {'params': classifier_params, 'lr': classifier_lr, 'momentum': 0.9, 'weight_decay': best_params['weight_decay']}\n",
    "        ])\n",
    "\n",
    "    # Create scheduler that handles parameter groups\n",
    "    if best_params['scheduler'] == 'cosine':\n",
    "        final_scheduler = optim.lr_scheduler.CosineAnnealingLR(final_optimizer, T_max=num_epochs)\n",
    "    elif best_params['scheduler'] == 'step':\n",
    "        final_scheduler = optim.lr_scheduler.StepLR(final_optimizer, step_size=5, gamma=0.5)\n",
    "    elif best_params['scheduler'] == 'exponential':\n",
    "        final_scheduler = optim.lr_scheduler.ExponentialLR(final_optimizer, gamma=0.9)\n",
    "\n",
    "        \n",
    "# If not fine-tuning, train only classifier head\n",
    "else:\n",
    "    final_model = create_model_with_params(best_params['dropout_rate'], freeze_backbone=True)\n",
    "    final_optimizer, final_scheduler = create_optimizer_scheduler(\n",
    "        final_model, best_params['optimizer'], best_params['learning_rate'],\n",
    "        best_params['weight_decay'], best_params['scheduler']\n",
    "    )\n",
    "\n",
    "final_model = final_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train final model for 20 epochs\n",
    "final_train_losses, final_train_accuracies, final_val_losses, final_val_accuracies, learning_rates = train_model(\n",
    "    final_model, final_train_loader, final_val_loader, criterion, final_optimizer, final_scheduler, num_epochs, device, fine_tune_backbone, patience=10, use_amp=True\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "final_test_accuracy, final_inference_time = evaluate_model(final_model, final_test_loader, device)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Train Accuracy: {final_train_accuracies[-1]:.2f}%\")\n",
    "print(f\"Validation Accuracy: {final_val_accuracies[-1]:.2f}%\")\n",
    "print(f\"Test Accuracy: {final_test_accuracy:.2f}%\")\n",
    "print(f\"Inference Time: {final_inference_time:.4f} ms per sample\")\n",
    "print(f\"Parameters: {count_parameters(final_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f4c39",
   "metadata": {},
   "source": [
    "## 4. Final Training and Evaluation\n",
    "\n",
    "Train the best hyperparameter configuration with optional backbone fine-tuning and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Hyperparameter tuning results\n",
    "plt.subplot(2, 3, 1)\n",
    "val_accuracies = [result['val_accuracy'] for result in tuning_results]\n",
    "plt.bar(range(len(val_accuracies)), val_accuracies, color='skyblue')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Hyperparameter Tuning Results')\n",
    "plt.xticks(range(len(val_accuracies)), [f'T{i+1}' for i in range(len(val_accuracies))])\n",
    "\n",
    "# Plot 2: Training and Validation Loss\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(final_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(final_val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training and Validation Accuracy\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(final_train_accuracies, label='Training Accuracy', color='green')\n",
    "plt.plot(final_val_accuracies, label='Validation Accuracy', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Final Performance Comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "datasets = ['Training', 'Validation', 'Test']\n",
    "accuracies = [final_train_accuracies[-1], final_val_accuracies[-1], final_test_accuracy]\n",
    "colors = ['green', 'orange', 'purple']\n",
    "bars = plt.bar(datasets, accuracies, color=colors, alpha=0.7)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Final Model Performance')\n",
    "plt.ylim([0, 100])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 5: Learning Rate Schedule\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot([lr[0] for lr in learning_rates], color='darkblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Training Stability\n",
    "plt.subplot(2, 3, 6)\n",
    "loss_diffs = [abs(train - val) for train, val in zip(final_train_losses, final_val_losses)]\n",
    "plt.plot(loss_diffs, color='darkred', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Difference')\n",
    "plt.title('Training Stability\\n(Train-Val Loss Gap)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
