{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a576c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# We use pickle to load the CIFAR-10 dataset\n",
    "import pickle\n",
    "\n",
    "# We use os and urllib to download and extract the dataset\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# We use tarfile to extract the downloaded tar.gz file\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b70d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manousos Kirkinis\\AppData\\Local\\Temp\\ipykernel_19236\\2339890072.py:4: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  data = pickle.load(f, encoding='bytes') # Load the batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 40000, Val: 10000, Test: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset manually with NumPy\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as f: # Open the file\n",
    "        data = pickle.load(f, encoding='bytes') # Load the batch\n",
    "\n",
    "    # Reshape images    \n",
    "    images = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) # Reshape and transpose to (N, H, W, C)\n",
    "    images = np.asarray(images, dtype=np.float32)  # Convert to NumPy array with float32\n",
    "    labels = np.array(data[b'labels']) # Get labels as NumPy array\n",
    "    return images, labels\n",
    "\n",
    "# Data augmentation functions\n",
    "def random_crop(img, crop_size=32, padding=4):\n",
    "\n",
    "    # Pad the image\n",
    "    img = np.pad(img, ((padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Random crop\n",
    "    top = np.random.randint(0, h - crop_size + 1)\n",
    "    left = np.random.randint(0, w - crop_size + 1)\n",
    "    \n",
    "    return img[top:top + crop_size, left:left + crop_size]\n",
    "\n",
    "def random_horizontal_flip(img):\n",
    "    if np.random.rand() > 0.5:\n",
    "        return np.flip(img, axis=1)\n",
    "    return img\n",
    "\n",
    "def random_erasing(img, p=0.5, sl=0.02, sh=0.4, r1=0.3):\n",
    "    if np.random.rand() > p:\n",
    "        return img\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    s = np.random.uniform(sl, sh) * img_h * img_w\n",
    "    r = np.random.uniform(r1, 1/r1)\n",
    "    w = int(np.sqrt(s / r))\n",
    "    h = int(np.sqrt(s * r))\n",
    "    left = np.random.randint(0, img_w - w + 1) if w < img_w else 0\n",
    "    top = np.random.randint(0, img_h - h + 1) if h < img_h else 0\n",
    "    img[top:top+h, left:left+w] = 0\n",
    "    return img\n",
    "\n",
    "def random_gaussian_noise(img, std=0.05):\n",
    "    noise = np.random.normal(0, std, img.shape)\n",
    "    return img + noise\n",
    "\n",
    "def augment_image(img):\n",
    "    img = random_crop(img)\n",
    "    img = random_horizontal_flip(img)\n",
    "    img = random_erasing(img)\n",
    "    img = random_gaussian_noise(img)\n",
    "    return img\n",
    "\n",
    "# Define data directory\n",
    "data_dir = './data/cifar-10-batches-py' \n",
    "\n",
    "# Download if not present\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    urllib.request.urlretrieve(url, './data/cifar-10.tar.gz')\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open('./data/cifar-10.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall('./data')\n",
    "\n",
    "# Load training data (5 batches of 10,000 images each)\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for i in range(1, 6):\n",
    "    images, labels = load_cifar10_batch(f'{data_dir}/data_batch_{i}')\n",
    "    train_images.append(images)\n",
    "    train_labels.append(labels)\n",
    "\n",
    "# We need to concatenate all training images because they are in separate batches\n",
    "train_images = np.concatenate(train_images) \n",
    "train_labels = np.concatenate(train_labels)  \n",
    "\n",
    "# Load test data\n",
    "test_images, test_labels = load_cifar10_batch(f'{data_dir}/test_batch')\n",
    "\n",
    "# Normalize images (mean/std for CIFAR-10)\n",
    "mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "std = np.array([0.2023, 0.1994, 0.2010])\n",
    "train_images = (train_images / 255.0 - mean) / std\n",
    "test_images = (test_images / 255.0 - mean) / std\n",
    "\n",
    "# Split train into train/val (80/20 rule)\n",
    "train_size = int(0.8 * len(train_images))\n",
    "val_images, val_labels = train_images[train_size:], train_labels[train_size:]\n",
    "train_images, train_labels = train_images[:train_size], train_labels[:train_size]\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "train_images = train_images.transpose(0, 3, 1, 2)  # (N, C, H, W)\n",
    "val_images = val_images.transpose(0, 3, 1, 2)\n",
    "test_images = test_images.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert to NumPy arrays for CPU\n",
    "train_images = np.asarray(train_images)\n",
    "val_images = np.asarray(val_images)\n",
    "test_images = np.asarray(test_images)\n",
    "train_labels = np.asarray(train_labels)\n",
    "val_labels = np.asarray(val_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "print(f'Train: {len(train_images)}, Val: {len(val_images)}, Test: {len(test_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6db103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layers from scratch using NumPy\n",
    "\n",
    "class Conv2DLayer:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "\n",
    "        # Initialize parameters for Conv2D layer (channels, filters, kernel size, stride, padding)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size).astype(np.float32) * 0.1\n",
    "        self.biases = np.zeros(out_channels, dtype=np.float32)\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dweights = np.zeros_like(self.weights)\n",
    "        self.dbiases = np.zeros_like(self.biases)\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "\n",
    "        # Get input dimensions (batch_size, in_channels, in_height, in_width)\n",
    "        batch_size, in_c, in_h, in_w = x.shape\n",
    "\n",
    "        # Calculate output dimensions and initialize output\n",
    "        out_h = (in_h + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_w = (in_w + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output = np.zeros((batch_size, self.out_channels, out_h, out_w), dtype=np.float32)\n",
    "        \n",
    "        # Padding input\n",
    "        # Apply padding to the input if specified. Padding adds zeros around the spatial dimensions (height and width) \n",
    "        # to control the output size after convolution and help preserve spatial information.\n",
    "        # np.pad uses ((batch_pad), (channel_pad), (height_pad), (width_pad)), so we pad only height and width.\n",
    "        if self.padding > 0:\n",
    "            x_padded = np.pad(x, ((0,0), (0,0), (self.padding,self.padding), (self.padding,self.padding)), mode='constant')\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        # Loop over batch, output channels, and output spatial dimensions (height, width)\n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "\n",
    "                        # Calculate the start and end indices for the current kernel window\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        # Extract the region of the input that the kernel will convolve with\n",
    "                        region = x_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                        # Perform the convolution: element-wise multiply region with kernel weights, sum, and add bias\n",
    "                        output[b, oc, i, j] = np.sum(region * self.weights[oc]) + self.biases[oc]\n",
    "        return output\n",
    "    \n",
    "    # Backward propagation\n",
    "    def backward(self, d_out):\n",
    "        # Get  output dimensions\n",
    "        batch_size, _, out_h, out_w = d_out.shape\n",
    "\n",
    "        # Initialize gradient for input\n",
    "        d_input = np.zeros_like(self.input)\n",
    "\n",
    "        # Use padding if necessary \n",
    "        if self.padding > 0:\n",
    "            d_input_padded = np.pad(d_input, ((0,0), (0,0), (self.padding,self.padding), (self.padding,self.padding)), mode='constant')\n",
    "        else:\n",
    "            d_input_padded = d_input\n",
    "        \n",
    "        # Reset gradients\n",
    "        self.dweights.fill(0)\n",
    "        self.dbiases.fill(0)\n",
    "        \n",
    "        # Padding input for backward pass (if needed)\n",
    "        if self.padding > 0:\n",
    "            x_padded = np.pad(self.input, ((0,0), (0,0), (self.padding,self.padding), (self.padding,self.padding)), mode='constant')\n",
    "        else:\n",
    "            x_padded = self.input\n",
    "        \n",
    "        # Loop over batch, output channels, and output spatial dimensions (height, width)\n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "\n",
    "                        # Calculate the start and end indices for the current kernel window\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        region = x_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "                        \n",
    "                        # Compute gradients for input, weights, and biases \n",
    "\n",
    "                        # Update input gradients and weights gradients\n",
    "                        d_input_padded[b, :, h_start:h_end, w_start:w_end] += self.weights[oc] * d_out[b, oc, i, j]\n",
    "                        self.dweights[oc] += region * d_out[b, oc, i, j]\n",
    "                # Update bias gradients\n",
    "                self.dbiases[oc] += np.sum(d_out[b, oc])\n",
    "        \n",
    "        # Remove padding from d_input if applied\n",
    "        if self.padding > 0:\n",
    "            d_input = d_input_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        else:\n",
    "            d_input = d_input_padded\n",
    "        return d_input\n",
    "\n",
    "# Activation Function Layer\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * (self.input > 0)\n",
    "\n",
    "# Pooling Layer\n",
    "class MaxPool2D:\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, channels, in_h, in_w = x.shape\n",
    "\n",
    "        # Calculate output dimensions and initialize output (like in Conv2DLayer)\n",
    "        out_h = (in_h - self.kernel_size) // self.stride + 1\n",
    "        out_w = (in_w - self.kernel_size) // self.stride + 1\n",
    "        output = np.zeros((batch_size, channels, out_h, out_w))\n",
    "        self.max_indices = np.zeros_like(output, dtype=int)\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        # Retrieve the region to pool\n",
    "                        region = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        output[b, c, i, j] = np.max(region)\n",
    "\n",
    "                        # Store the index of the max value for backpropagation\n",
    "                        self.max_indices[b, c, i, j] = np.argmax(region)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        batch_size, channels, out_h, out_w = d_out.shape\n",
    "        d_input = np.zeros_like(self.input)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        # Get the index of the max value from the forward pass\n",
    "                        max_idx = self.max_indices[b, c, i, j]\n",
    "                        d_input[b, c, h_start:h_end, w_start:w_end].flat[max_idx] += d_out[b, c, i, j]\n",
    "\n",
    "        # We return the gradient with respect to the input of the pooling layer\n",
    "        return d_input\n",
    "\n",
    "# Fully Connected Layer\n",
    "class FC:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weights = np.random.randn(out_features, in_features).astype(np.float32) * 0.1\n",
    "        self.biases = np.zeros(out_features, dtype=np.float32)\n",
    "        self.dweights = np.zeros_like(self.weights)\n",
    "        self.dbiases = np.zeros_like(self.biases)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights.T) + self.biases\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        self.dweights = np.dot(d_out.T, self.input)\n",
    "        self.dbiases = np.sum(d_out, axis=0)\n",
    "        return np.dot(d_out, self.weights)\n",
    "\n",
    "# Dropout Layer\n",
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True  # Set to False for inference\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training and self.p > 0:\n",
    "            self.mask = np.random.rand(*x.shape) > self.p\n",
    "            return x * self.mask / (1 - self.p)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        if self.training and self.p > 0:\n",
    "            return d_out * self.mask / (1 - self.p)\n",
    "        return d_out\n",
    "\n",
    "class CustomCNN:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.conv1 = Conv2DLayer(3, 32, 3, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2D(2, 2)\n",
    "        self.conv2 = Conv2DLayer(32, 64, 3, padding=1)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2D(2, 2)\n",
    "        self.fc = FC(64 * 8 * 8, 10)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, x):\n",
    "        x = self.pool1.forward(self.relu1.forward(self.conv1.forward(x)))\n",
    "        x = self.pool2.forward(self.relu2.forward(self.conv2.forward(x)))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout.forward(self.fc.forward(x))\n",
    "        return x\n",
    "    \n",
    "    # Backward propagation\n",
    "    def backward(self, d_out):\n",
    "        d_out = self.dropout.backward(d_out)\n",
    "        d_out = self.fc.backward(d_out)\n",
    "        d_out = d_out.reshape(d_out.shape[0], 64, 8, 8)\n",
    "        d_out = self.pool2.backward(d_out)\n",
    "        d_out = self.conv2.backward(self.relu2.backward(d_out))\n",
    "        d_out = self.pool1.backward(d_out)\n",
    "        d_out = self.conv1.backward(self.relu1.backward(d_out))\n",
    "    \n",
    "    # Update parameters with learning rate (gradient descent) and weight decay\n",
    "    def update_params(self, lr, weight_decay=0):\n",
    "        self.conv1.weights -= lr * (self.conv1.dweights + weight_decay * self.conv1.weights)\n",
    "        self.conv1.biases -= lr * self.conv1.dbiases\n",
    "        self.conv2.weights -= lr * (self.conv2.dweights + weight_decay * self.conv2.weights)\n",
    "        self.conv2.biases -= lr * self.conv2.dbiases\n",
    "        self.fc.weights -= lr * (self.fc.dweights + weight_decay * self.fc.weights)\n",
    "        self.fc.biases -= lr * self.fc.dbiases\n",
    "    \n",
    "    def set_training(self, training=True):\n",
    "        self.dropout.training = training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c3a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (Cross-Entropy)\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    batch_size = y_pred.shape[0]\n",
    "\n",
    "    # Clip predictions to avoid log(0) error\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    log_probs = -np.log(y_pred[range(batch_size), y_true])\n",
    "    return np.mean(log_probs)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Gradient of loss\n",
    "def d_cross_entropy_loss(y_pred, y_true):\n",
    "    batch_size = y_pred.shape[0]\n",
    "    grad = y_pred.copy()\n",
    "\n",
    "    # Subtract 1 from the predicted probabilities of the true classes \n",
    "    # to compute the gradient of the cross-entropy loss with respect to the predictions (softmax output).\n",
    "    grad[range(batch_size), y_true] -= 1\n",
    "    return grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa181a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     40\u001b[39m d_loss = d_cross_entropy_loss(probs, batch_y)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[32m     44\u001b[39m model.update_params(lr, wd)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 238\u001b[39m, in \u001b[36mCustomCNN.backward\u001b[39m\u001b[34m(self, d_out)\u001b[39m\n\u001b[32m    236\u001b[39m d_out = \u001b[38;5;28mself\u001b[39m.conv2.backward(\u001b[38;5;28mself\u001b[39m.relu2.backward(d_out))\n\u001b[32m    237\u001b[39m d_out = \u001b[38;5;28mself\u001b[39m.pool1.backward(d_out)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m d_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrelu1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mConv2DLayer.backward\u001b[39m\u001b[34m(self, d_out)\u001b[39m\n\u001b[32m     98\u001b[39m         \u001b[38;5;66;03m# Compute gradients for input, weights, and biases \u001b[39;00m\n\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m         \u001b[38;5;66;03m# Update input gradients and weights gradients\u001b[39;00m\n\u001b[32m    101\u001b[39m         d_input_padded[b, :, h_start:h_end, w_start:w_end] += \u001b[38;5;28mself\u001b[39m.weights[oc] * d_out[b, oc, i, j]\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         \u001b[38;5;28mself\u001b[39m.dweights[oc] += region * d_out[b, oc, i, j]\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Update bias gradients\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.dbiases[oc] += np.sum(d_out[b, oc])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop with fixed hyperparameters\n",
    "import random\n",
    "import time\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Fixed hyperparameters for performance\n",
    "lr = 0.01  # Learning rate\n",
    "bs = 128  # Batch size\n",
    "dr = 0.5  # Dropout rate\n",
    "wd = 1e-4  # Weight decay\n",
    "\n",
    "model = CustomCNN(dropout_rate=dr)\n",
    "\n",
    "num_epochs = 5  # Number of epochs\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Shuffle train data\n",
    "    indices = np.random.permutation(len(train_images))\n",
    "    train_images_shuf = train_images[indices]\n",
    "    train_labels_shuf = train_labels[indices]\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(0, len(train_images_shuf), bs):\n",
    "        batch_x = train_images_shuf[i:i+bs]\n",
    "        batch_y = train_labels_shuf[i:i+bs]\n",
    "        \n",
    "        # Apply data augmentation to training batch\n",
    "        batch_x_aug = []\n",
    "        for img in batch_x:\n",
    "            img_hwc = img.transpose(1, 2, 0)  # (H, W, C)\n",
    "            img_aug = augment_image(img_hwc)\n",
    "            img_aug_chw = img_aug.transpose(2, 0, 1)  # (C, H, W)\n",
    "            batch_x_aug.append(img_aug_chw)\n",
    "        batch_x = np.array(batch_x_aug)\n",
    "        \n",
    "        # Forward\n",
    "        out = model.forward(batch_x)\n",
    "        probs = softmax(out)\n",
    "        loss = cross_entropy_loss(probs, batch_y)\n",
    "        \n",
    "        # Backward\n",
    "        d_loss = d_cross_entropy_loss(probs, batch_y)\n",
    "        model.backward(d_loss)\n",
    "        \n",
    "        # Update\n",
    "        model.update_params(lr, wd)\n",
    "        \n",
    "        running_loss += loss * len(batch_x)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        correct += np.sum(preds == batch_y)\n",
    "        total += len(batch_x)\n",
    "    \n",
    "    train_loss = running_loss / len(train_images)\n",
    "    train_acc = 100 * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation (disable dropout, no augmentation)\n",
    "    model.set_training(False)\n",
    "    val_bs = 64\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    for j in range(0, len(val_images), val_bs):\n",
    "        val_batch_x = val_images[j:j+val_bs]\n",
    "        val_batch_y = val_labels[j:j+val_bs]\n",
    "        val_out = model.forward(val_batch_x)\n",
    "        val_probs = softmax(val_out)\n",
    "        val_loss = cross_entropy_loss(val_probs, val_batch_y)\n",
    "        val_preds = np.argmax(val_probs, axis=1)\n",
    "        val_running_loss += val_loss * len(val_batch_x)\n",
    "        val_correct += np.sum(val_preds == val_batch_y)\n",
    "        val_total += len(val_batch_x)\n",
    "    \n",
    "    model.set_training(True)  # Re-enable for next epoch\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_images)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Time: {epoch_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the CNN on the test dataset (unknown data)\n",
    "\n",
    "# Load test data\n",
    "test_images, test_labels = load_cifar10_batch(f'{data_dir}/test_batch')\n",
    "\n",
    "# Normalize test images\n",
    "test_images = (test_images / 255.0 - mean) / std\n",
    "test_images = test_images.transpose(0, 3, 1, 2)  # (N, C, H, W)\n",
    "\n",
    "model.set_training(False)  # Disable dropout for inference\n",
    "\n",
    "# Evaluate on test set\n",
    "test_out = model.forward(test_images)\n",
    "test_probs = softmax(test_out)\n",
    "test_preds = np.argmax(test_probs, axis=1)\n",
    "\n",
    "test_acc = 100 * np.sum(test_preds == test_labels) / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[f'Class {i}' for i in range(10)], yticklabels=[f'Class {i}' for i in range(10)])\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "# Show some example predictions\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Denormalize for display\n",
    "test_images_display = test_images.transpose(0, 2, 3, 1) * std + mean\n",
    "test_images_display = np.clip(test_images_display, 0, 1)\n",
    "\n",
    "# Select 5 random examples\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(test_images), 5, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, idx in enumerate(indices):\n",
    "    img = test_images_display[idx]\n",
    "    true_label = class_names[test_labels[idx]]\n",
    "    pred_label = class_names[test_preds[idx]]\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b358057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# Plot training vs validation curves using Seaborn\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses) + 1),\n",
    "    'Train Loss': train_losses,\n",
    "    'Val Loss': val_losses,\n",
    "    'Train Acc': train_accs,\n",
    "    'Val Acc': val_accs\n",
    "})\n",
    "\n",
    "df_melted_loss = df.melt(id_vars='Epoch', value_vars=['Train Loss', 'Val Loss'], var_name='Type', value_name='Loss')\n",
    "df_melted_acc = df.melt(id_vars='Epoch', value_vars=['Train Acc', 'Val Acc'], var_name='Type', value_name='Accuracy')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=df_melted_loss, x='Epoch', y='Loss', hue='Type')\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=df_melted_acc, x='Epoch', y='Accuracy', hue='Type')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
