{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.amp as amp\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# We use pickle to load the CIFAR-10 dataset\n",
    "import pickle\n",
    "\n",
    "# We use os and urllib to download and extract the dataset\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# We use tarfile to extract the downloaded tar.gz file\n",
    "import tarfile\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset manually with PyTorch\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as f: # Open the file\n",
    "        data = pickle.load(f, encoding='bytes') # Load the batch\n",
    "\n",
    "    # Reshape images    \n",
    "    images = data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) # Reshape and transpose to (N, H, W, C)\n",
    "    images = torch.tensor(images, dtype=torch.float32)  # Convert to PyTorch tensor with float32\n",
    "    labels = torch.tensor(data[b'labels'], dtype=torch.long) # Get labels as PyTorch tensor\n",
    "    return images, labels\n",
    "\n",
    "# Data augmentation functions\n",
    "def random_crop(img, crop_size=32, padding=4):\n",
    "    # Pad the image\n",
    "    img = F.pad(img.permute(2, 0, 1), (padding, padding, padding, padding), mode='constant', value=0).permute(1, 2, 0)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Random crop\n",
    "    top = torch.randint(0, h - crop_size + 1, (1,)).item()\n",
    "    left = torch.randint(0, w - crop_size + 1, (1,)).item()\n",
    "    \n",
    "    return img[top:top + crop_size, left:left + crop_size]\n",
    "\n",
    "def random_horizontal_flip(img):\n",
    "    if torch.rand(1) > 0.5:\n",
    "        return torch.flip(img, dims=[1])\n",
    "    return img\n",
    "\n",
    "def random_erasing(img, p=0.5, sl=0.02, sh=0.4, r1=0.3):\n",
    "    if torch.rand(1) > p:\n",
    "        return img\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    s = torch.rand(1).uniform_(sl, sh) * img_h * img_w\n",
    "    r = torch.rand(1).uniform_(r1, 1/r1)\n",
    "    w = int(torch.sqrt(s / r))\n",
    "    h = int(torch.sqrt(s * r))\n",
    "    left = torch.randint(0, img_w - w + 1, (1,)).item() if w < img_w else 0\n",
    "    top = torch.randint(0, img_h - h + 1, (1,)).item() if h < img_h else 0\n",
    "    img[top:top+h, left:left+w] = 0\n",
    "    return img\n",
    "\n",
    "def random_gaussian_noise(img, std=0.05):\n",
    "    noise = torch.randn_like(img) * std\n",
    "    return img + noise\n",
    "\n",
    "def augment_image(img):\n",
    "    img = random_crop(img)\n",
    "    img = random_horizontal_flip(img)\n",
    "    img = random_erasing(img)\n",
    "    img = random_gaussian_noise(img)\n",
    "    return img\n",
    "\n",
    "# Define data directory\n",
    "data_dir = './data/cifar-10-batches-py' \n",
    "\n",
    "# Download if not present\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    urllib.request.urlretrieve(url, './data/cifar-10.tar.gz')\n",
    "\n",
    "    # Extract the tar file\n",
    "    with tarfile.open('./data/cifar-10.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall('./data')\n",
    "\n",
    "# Load training data (5 batches of 10,000 images each)\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for i in range(1, 6):\n",
    "    images, labels = load_cifar10_batch(f'{data_dir}/data_batch_{i}')\n",
    "    train_images.append(images)\n",
    "    train_labels.append(labels)\n",
    "\n",
    "# We need to concatenate all training images because they are in separate batches\n",
    "train_images = torch.cat(train_images) \n",
    "train_labels = torch.cat(train_labels)  \n",
    "\n",
    "# Load test data\n",
    "test_images, test_labels = load_cifar10_batch(f'{data_dir}/test_batch')\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalize images (mean/std for CIFAR-10)\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "std = torch.tensor([0.2023, 0.1994, 0.2010])\n",
    "train_images = (train_images / 255.0 - mean) / std\n",
    "test_images = (test_images / 255.0 - mean) / std\n",
    "\n",
    "# Convert to PyTorch tensors (N, C, H, W)\n",
    "train_images = train_images.permute(0, 3, 1, 2)\n",
    "test_images = test_images.permute(0, 3, 1, 2)\n",
    "\n",
    "train_images = train_images.to(device)\n",
    "test_images = test_images.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "print(f'Train: {len(train_images)}, Test: {len(test_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN using PyTorch nn.Module\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, filters1=32, filters2=64, filters3=128, filters4=256, dropout_rate=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, filters1, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(filters1)\n",
    "        self.conv2 = nn.Conv2d(filters1, filters2, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(filters2)\n",
    "        self.conv3 = nn.Conv2d(filters2, filters3, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(filters3)\n",
    "        self.conv4 = nn.Conv2d(filters3, filters4, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(filters4)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(filters4 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (Cross-Entropy)\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    batch_size = y_pred.shape[0]\n",
    "\n",
    "    # Clip predictions to avoid log(0) error\n",
    "    y_pred = torch.clamp(y_pred, 1e-7, 1 - 1e-7)\n",
    "    log_probs = -torch.log(y_pred[torch.arange(batch_size, device=y_pred.device), y_true])\n",
    "    return torch.mean(log_probs)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    exp_x = torch.exp(x - torch.max(x, dim=1, keepdims=True)[0])\n",
    "    return exp_x / torch.sum(exp_x, dim=1, keepdims=True)\n",
    "\n",
    "# Gradient of loss\n",
    "def d_cross_entropy_loss(y_pred, y_true):\n",
    "    batch_size = y_pred.shape[0]\n",
    "    grad = y_pred.clone()\n",
    "\n",
    "    # Subtract 1 from the predicted probabilities of the true classes \n",
    "    # to compute the gradient of the cross-entropy loss with respect to the predictions (softmax output).\n",
    "    grad[torch.arange(batch_size, device=y_pred.device), y_true] -= 1\n",
    "    return grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search with Grid Search and Stratified Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define hyperparameter lists\n",
    "lr_list = [0.0001, 0.0005, 0.001, 0.005] # learning rates\n",
    "bs_list = [32, 64] #batch sizes\n",
    "dr_list = [0.2, 0.3, 0.4] # dropout rates\n",
    "wd_list = [0, 1e-5, 1e-4] # weight decay\n",
    "\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "total_combos = len(lr_list) * len(bs_list) * len(dr_list) * len(wd_list)\n",
    "print(f\"Total combinations: {total_combos}\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "combo_count = 0\n",
    "for lr in lr_list:\n",
    "    for bs in bs_list:\n",
    "        for dr in dr_list:\n",
    "            for wd in wd_list:\n",
    "                combo_count += 1\n",
    "                print(f\"Testing combo {combo_count}/{total_combos}: lr={lr}, bs={bs}, dr={dr}, wd={wd}\")\n",
    "                \n",
    "                fold_accs = []\n",
    "                for fold, (train_idx, val_idx) in enumerate(skf.split(train_images, train_labels.cpu().numpy())):\n",
    "                    # Split data for this fold\n",
    "                    fold_train_images = train_images[train_idx]\n",
    "                    fold_train_labels = train_labels[train_idx]\n",
    "                    fold_val_images = train_images[val_idx]\n",
    "                    fold_val_labels = train_labels[val_idx]\n",
    "                    \n",
    "                    torch.manual_seed(42)\n",
    "                    model = CustomCNN(dropout_rate=dr).to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                    scaler = torch.amp.GradScaler()\n",
    "                    \n",
    "                    # Train for 5 epochs (increased for deeper model)\n",
    "                    for epoch in range(5):\n",
    "                        model.train()\n",
    "                        indices = torch.randperm(len(fold_train_images))\n",
    "                        fold_train_images_shuf = fold_train_images[indices]\n",
    "                        fold_train_labels_shuf = fold_train_labels[indices]\n",
    "                        \n",
    "                        running_loss = 0.0\n",
    "                        correct = 0\n",
    "                        total = 0\n",
    "                        \n",
    "                        for i in range(0, len(fold_train_images_shuf), bs):\n",
    "                            batch_x = fold_train_images_shuf[i:i+bs]\n",
    "                            batch_y = fold_train_labels_shuf[i:i+bs]\n",
    "                            \n",
    "                            # Apply data augmentation\n",
    "                            batch_x_aug = torch.zeros_like(batch_x)\n",
    "                            for j in range(len(batch_x)):\n",
    "                                img = batch_x[j].permute(1, 2, 0)\n",
    "                                img_aug = augment_image(img)\n",
    "                                batch_x_aug[j] = img_aug.permute(2, 0, 1)\n",
    "                            batch_x = batch_x_aug\n",
    "                            \n",
    "                            with amp.autocast('cuda'):\n",
    "                                out = model(batch_x)\n",
    "                                loss = F.cross_entropy(out, batch_y, label_smoothing=0.1)\n",
    "                            \n",
    "                            # Backward \n",
    "                            scaler.scale(loss).backward()\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            optimizer.zero_grad()\n",
    "                            \n",
    "                            running_loss += loss.item() * len(batch_x)\n",
    "                            preds = torch.argmax(out, dim=1)\n",
    "                            correct += torch.sum(preds == batch_y).item()\n",
    "                            total += len(batch_x)\n",
    "                    \n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    val_correct = 0\n",
    "                    val_total = 0\n",
    "                    with torch.no_grad():\n",
    "                        for j in range(0, len(fold_val_images), 64):\n",
    "                            val_batch_x = fold_val_images[j:j+64]\n",
    "                            val_batch_y = fold_val_labels[j:j+64]\n",
    "                            val_out = model(val_batch_x)\n",
    "                            val_preds = torch.argmax(val_out, dim=1)\n",
    "                            val_correct += torch.sum(val_preds == val_batch_y).item()\n",
    "                            val_total += len(val_batch_x)\n",
    "                    \n",
    "                    fold_acc = 100 * val_correct / val_total\n",
    "                    fold_accs.append(fold_acc)\n",
    "                \n",
    "                avg_val_acc = sum(fold_accs) / len(fold_accs)\n",
    "                print(f\"Average Val Acc: {avg_val_acc:.2f}%\")\n",
    "                \n",
    "                if avg_val_acc > best_val_acc:\n",
    "                    best_val_acc = avg_val_acc\n",
    "                    best_params = {'lr': lr, 'bs': bs, 'dr': dr, 'wd': wd}\n",
    "\n",
    "print(f\"Best combo: {best_params}, Avg Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Set the best hyperparameters\n",
    "lr = best_params['lr']\n",
    "bs = best_params['bs']\n",
    "dr = best_params['dr']\n",
    "wd = best_params['wd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa181a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with PyTorch\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "# Using best hyperparameters from search\n",
    "print(f\"Using hyperparameters: lr={lr}, bs={bs}, dr={dr}, wd={wd}\")\n",
    "\n",
    "model = CustomCNN(dropout_rate=dr).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "num_epochs = 30\n",
    "patience = 5  # Early stopping\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "# Use StratifiedKFold for train/val split\n",
    "skf_train = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(skf_train.split(train_images, train_labels.cpu().numpy()))\n",
    "\n",
    "val_images = train_images[val_idx]\n",
    "val_labels = train_labels[val_idx]\n",
    "train_images_eff = train_images[train_idx]\n",
    "train_labels_eff = train_labels[train_idx]\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    # Shuffle train data\n",
    "    indices = torch.randperm(len(train_images_eff))\n",
    "    train_images_shuf = train_images_eff[indices]\n",
    "    train_labels_shuf = train_labels_eff[indices]\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(0, len(train_images_shuf), bs):\n",
    "        batch_x = train_images_shuf[i:i+bs]\n",
    "        batch_y = train_labels_shuf[i:i+bs]\n",
    "        \n",
    "        # Apply data augmentation to training batch\n",
    "        batch_x_aug = torch.zeros_like(batch_x)\n",
    "        for j in range(len(batch_x)):\n",
    "            img = batch_x[j].permute(1, 2, 0)  # (H, W, C)\n",
    "            img_aug = augment_image(img)\n",
    "            batch_x_aug[j] = img_aug.permute(2, 0, 1)  # (C, H, W)\n",
    "        batch_x = batch_x_aug\n",
    "        \n",
    "        # Forward with mixed precision\n",
    "        with amp.autocast('cuda'):\n",
    "            out = model(batch_x)\n",
    "            loss = F.cross_entropy(out, batch_y, label_smoothing=0.1)\n",
    "        \n",
    "        # Backward\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item() * len(batch_x)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        correct += torch.sum(preds == batch_y).item()\n",
    "        total += len(batch_x)\n",
    "    \n",
    "    train_loss = running_loss / len(train_images_eff)\n",
    "    train_acc = 100 * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(val_images), 64):\n",
    "            val_batch_x = val_images[j:j+64]\n",
    "            val_batch_y = val_labels[j:j+64]\n",
    "            val_out = model(val_batch_x)\n",
    "            val_loss = F.cross_entropy(val_out, val_batch_y)\n",
    "            val_preds = torch.argmax(val_out, dim=1)\n",
    "            val_running_loss += val_loss.item() * len(val_batch_x)\n",
    "            val_correct += torch.sum(val_preds == val_batch_y).item()\n",
    "            val_total += len(val_batch_x)\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_images)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the CNN on the test dataset (unknown data)\n",
    "\n",
    "# Load test data\n",
    "test_images, test_labels = load_cifar10_batch(f'{data_dir}/test_batch')\n",
    "\n",
    "# Normalize test images\n",
    "test_images = (test_images / 255.0 - mean) / std\n",
    "test_images = test_images.permute(0, 3, 1, 2)  # (N, C, H, W)\n",
    "test_images = test_images.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "model.eval()  # Disable dropout for inference\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    test_out = model(test_images)\n",
    "    test_preds = torch.argmax(test_out, dim=1)\n",
    "\n",
    "test_acc = 100 * torch.sum(test_preds == test_labels).item() / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels.cpu().numpy(), test_preds.cpu().numpy())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[f'Class {i}' for i in range(10)], yticklabels=[f'Class {i}' for i in range(10)])\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(test_labels.cpu().numpy(), test_preds.cpu().numpy()))\n",
    "\n",
    "# Show some example predictions\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Denormalize for display\n",
    "test_images_display = test_images.cpu().permute(0, 2, 3, 1) * std + mean\n",
    "test_images_display = torch.clamp(test_images_display, 0, 1)\n",
    "\n",
    "# Select 5 random examples\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(test_images))[:5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, idx in enumerate(indices):\n",
    "    img = test_images_display[idx].cpu().numpy()\n",
    "    true_label = class_names[test_labels[idx].item()]\n",
    "    pred_label = class_names[test_preds[idx].item()]\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b358057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# Plot training vs validation curves using Seaborn\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses) + 1),\n",
    "    'Train Loss': [float(x) for x in train_losses],\n",
    "    'Val Loss': [float(x) for x in val_losses],\n",
    "    'Train Acc': [float(x) for x in train_accs],\n",
    "    'Val Acc': [float(x) for x in val_accs]\n",
    "})\n",
    "\n",
    "df_melted_loss = df.melt(id_vars='Epoch', value_vars=['Train Loss', 'Val Loss'], var_name='Type', value_name='Loss')\n",
    "df_melted_acc = df.melt(id_vars='Epoch', value_vars=['Train Acc', 'Val Acc'], var_name='Type', value_name='Accuracy')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=df_melted_loss, x='Epoch', y='Loss', hue='Type')\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=df_melted_acc, x='Epoch', y='Accuracy', hue='Type')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "pip-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
