{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f74905",
   "metadata": {},
   "source": [
    "# MNIST MLP Classification Project\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for MNIST digit classification using PyTorch:\n",
    "\n",
    "1. **Data Loading & EDA**: Load MNIST dataset and explore the data\n",
    "2. **Model Definition**: Multi-Layer Perceptron with configurable hidden layers\n",
    "3. **Hyperparameter Tuning**: Use RandomizedSearchCV with Stratified K-Fold\n",
    "4. **Training & Monitoring**: Train with best params, monitor loss/accuracy curves\n",
    "5. **Evaluation**: Comprehensive testing with metrics and visualizations\n",
    "\n",
    "**Key Features:**\n",
    "- Hyperparameter tuning for architecture, learning rate, batch size, optimizer\n",
    "- Training monitoring with validation curves\n",
    "- Detailed evaluation metrics (accuracy, precision, recall, confusion matrix)\n",
    "- Sample predictions visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c72770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Import data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import scikit-learn libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f60b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # transform images to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # normalize the images 0.5 mean and 0.5 std\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders which will help in batching the data for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9178a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"Classes: {list(range(10))}\")\n",
    "\n",
    "# Class distribution\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "test_labels = [label for _, label in test_dataset]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_labels, bins=10, alpha=0.7, edgecolor='black')\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_labels, bins=10, alpha=0.7, edgecolor='black')\n",
    "plt.title('Test Set Class Distribution')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    img, label = train_dataset[i]\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Training Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "# Architecture: Input (784) -> Hidden layers (128, 64) -> Output (10 classes with softmax)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64], num_classes=10, dropout_prob=0.2):\n",
    "        super(MLP, self).__init__() # Initialize the nn.Module\n",
    "        layers = []\n",
    "        in_size = input_size # Input layer size\n",
    "        for h_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, h_size))  # Fully connected layer\n",
    "            layers.append(nn.BatchNorm1d(h_size))  # Batch normalization\n",
    "            layers.append(nn.ReLU())  # Activation function\n",
    "            layers.append(nn.Dropout(dropout_prob))  # Dropout\n",
    "            in_size = h_size # Update input size for next layer\n",
    "        layers.append(nn.Linear(in_size, num_classes))  # Output layer\n",
    "        self.layers = nn.Sequential(*layers)  # Stack all layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten 28x28 images to 784 vector\n",
    "        return self.layers(x)  # Forward pass through all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf01a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn wrapper Allows PyTorch model to work with sklearn's hyperparameter tuning tools\n",
    "class TorchMLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_sizes=[128, 64], learning_rate=0.01, epochs=10, batch_size=64, optimizer='adam', dropout_prob=0.2):\n",
    "        self.hidden_sizes = hidden_sizes \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.model = None \n",
    "        self.criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize model with current hyperparameters\n",
    "        self.model = MLP(hidden_sizes=self.hidden_sizes, dropout_prob=self.dropout_prob)\n",
    "        \n",
    "        # Select optimizer\n",
    "        if self.optimizer == 'adam':\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer == 'sgd':\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Convert to PyTorch tensors and create DataLoader\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor) \n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()  # Clear gradients\n",
    "                outputs = self.model(inputs)  # Forward pass\n",
    "                loss = self.criterion(outputs, labels)  # Compute loss\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Update weights\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions (no gradients needed)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor) # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
    "        return predicted.numpy()\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Return accuracy for sklearn compatibility\n",
    "        predictions = self.predict(X)\n",
    "        return accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for hyperparameter tuning (use subset for speed)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(5000)) # Use 5k samples for tuning  \n",
    "train_loader_subset = DataLoader(train_subset, batch_size=len(train_subset), shuffle=False) # Load all samples in one batch\n",
    "\n",
    "X_train, y_train = next(iter(train_loader_subset)) # iterate once to get all data\n",
    "X_train = X_train.numpy()\n",
    "y_train = y_train.numpy()\n",
    "\n",
    "# Flatten for MLP input (28x28 -> 784)\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1919ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with RandomizedSearchCV using Stratified K-Fold\n",
    "param_distributions = {\n",
    "    'hidden_sizes': [[128], [128, 64]],  # Network architectures\n",
    "    'learning_rate': [0.01, 0.1],  # Learning rates to try\n",
    "    'epochs': [3, 5],  # Number of training epochs\n",
    "    'batch_size': [64],  # Batch sizes for mini-batch training\n",
    "    'optimizer': ['adam', 'sgd'],  # Optimization algorithms\n",
    "    'dropout_prob': [0.2, 0.3]  # Dropout probabilities\n",
    "}\n",
    "\n",
    "# No pipeline needed since data is pre-normalized\n",
    "mlp = TorchMLPClassifier()\n",
    "stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    mlp, \n",
    "    param_distributions, \n",
    "    n_iter=5,  # Try 5 random combinations\n",
    "    cv=stratified_kfold, \n",
    "    scoring='accuracy', \n",
    "    verbose=1, \n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dade689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train full model with best parameters and plot loss/accuracy curves\n",
    "best_params = random_search.best_params_\n",
    "model = MLP(hidden_sizes=best_params['hidden_sizes'])\n",
    "\n",
    "# Initialize optimizer with best parameters\n",
    "if best_params['optimizer'] == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "elif best_params['optimizer'] == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Split full training data into train/validation for monitoring using random_split because we use pytorch dataset \n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size]) \n",
    "train_loader = DataLoader(train_subset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "epochs = best_params['epochs']\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop with validation monitoring\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "    train_accuracies.append(correct_train / total_train)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    val_losses.append(epoch_val_loss / len(val_loader))\n",
    "    val_accuracies.append(correct_val / total_val)\n",
    "\n",
    "# Step the scheduler\n",
    "scheduler.step()\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_title('Loss Curves')\n",
    "\n",
    "ax2.plot(train_accuracies, label='Training Accuracy')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_title('Accuracy Curves')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76344f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (completely unseen data)\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668133b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation metrics\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Classification report: precision, recall, f1-score per class\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Confusion matrix: shows prediction errors\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Sample predictions: visual check of model performance\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    img, label = test_dataset[i]\n",
    "    pred = model(img.unsqueeze(0)).argmax().item()\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'True: {label}, Pred: {pred}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
