{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78286696",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/Housing-Dataset.csv')\n",
    "#print(data.head())\n",
    "print(data.info())\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA (Exploratory Data Analysis for understanding the data)\n",
    "print(data['ocean_proximity'].value_counts())\n",
    "\n",
    "data['ocean_proximity'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Ocean Proximity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ocean Proximity')\n",
    "plt.show()\n",
    "\n",
    "axes = data.hist(bins=50, figsize=(20,15))\n",
    "\n",
    "# Add xlabel and ylabel to each subplot\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.suptitle(\"Histograms of Housing Dataset Features\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop('median_house_value', axis=1)\n",
    "y = data['median_house_value']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07be5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Creation\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_train and y_train into a single DataFrame for EDA\n",
    "eda_train = X_train.copy()\n",
    "eda_train['median_house_value'] = y_train\n",
    "# Apply preprocessing to X_train\n",
    "eda_preprocessed_X = preprocessor.fit_transform(eda_train.drop('median_house_value', axis=1))\n",
    "\n",
    "# Get feature names after one-hot encoding\n",
    "ohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "all_feature_names = np.concatenate([numerical_cols, ohe_feature_names])\n",
    "\n",
    "# Convert to DataFrame\n",
    "eda_preprocessed_df = pd.DataFrame(eda_preprocessed_X, columns=all_feature_names, index=X_train.index)\n",
    "\n",
    "# Add target column\n",
    "eda_preprocessed_df['median_house_value'] = y_train.values\n",
    "\n",
    "# Compute correlation\n",
    "corr_matrix = eda_preprocessed_df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    annot_kws={\"size\": 8}\n",
    ")\n",
    "plt.title(\"Correlation Matrix of Preprocessed Features Including Target\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d295400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Adding new features\n",
    "def feature_engineering(X):\n",
    "    X = X.copy()\n",
    "    X['rooms_per_household'] = X['total_rooms'] / X['households']\n",
    "    X['bedrooms_per_room'] = X['total_bedrooms'] / X['total_rooms']\n",
    "    X['population_per_household'] = X['population'] / X['households']\n",
    "\n",
    "    drop_cols = ['total_rooms', 'total_bedrooms', 'population', 'households']\n",
    "    X = X.drop(columns=drop_cols)\n",
    "\n",
    "    return X\n",
    "\n",
    "# EDA with new features\n",
    "\n",
    "# Apply feature engineering\n",
    "eda_train_second = feature_engineering(X_train.copy())\n",
    "\n",
    "# Identify numeric and categorical columns after feature engineering\n",
    "numeric_cols_second = eda_train_second.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols_second = eda_train_second.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Build a new preprocessor (the same as before but for the new feature set)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor_second = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols_second),\n",
    "    ('cat', categorical_transformer, categorical_cols_second)\n",
    "])\n",
    "\n",
    "# Transform data\n",
    "eda_preprocessed_X_second = preprocessor_second.fit_transform(eda_train_second)\n",
    "\n",
    "# Get updated column names\n",
    "ohe_feature_names = preprocessor_second.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols_second)\n",
    "all_feature_names_second = np.concatenate([numeric_cols_second, ohe_feature_names])\n",
    "\n",
    "# Create DataFrame\n",
    "eda_preprocessed_df_second = pd.DataFrame(\n",
    "    eda_preprocessed_X_second, \n",
    "    columns=all_feature_names_second, \n",
    "    index=X_train.index\n",
    ")\n",
    "eda_preprocessed_df_second['median_house_value'] = y_train.values\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix_second = eda_preprocessed_df_second.corr()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(\n",
    "    corr_matrix_second,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    annot_kws={\"size\": 8}\n",
    ")\n",
    "plt.title(\"Correlation Matrix with Engineered Features\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c34d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_adder = FunctionTransformer(feature_engineering) # Feature engineering transformer\n",
    "\n",
    "# Full pipeline with feature engineering\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('feature_adder', feature_adder),\n",
    "    ('preprocessor', preprocessor_second),\n",
    "    ('model', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_distributions = {\n",
    "    'model__max_depth': [None, 5, 10, 15, 20, 25],\n",
    "    'model__min_samples_split': [2, 5, 10, 15],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 6]\n",
    "}  \n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Randomized Search\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Training Time (seconds):\", end_time - start_time)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test MAE:\", mae)\n",
    "print(\"Test RÂ²:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names):\n",
    "    # Check if model has feature_importances_ attribute\n",
    "    if hasattr(model.named_steps['model'], 'feature_importances_'):\n",
    "        importances = model.named_steps['model'].feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.bar(range(len(importances)), importances[indices])\n",
    "        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"The model does not have feature importances.\")\n",
    "\n",
    "# get feature importances\n",
    "plot_feature_importances(best_model, all_feature_names_second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59ce43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
